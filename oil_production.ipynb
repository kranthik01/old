{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6bdef76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow has access to the following devices:\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check for TensorFlow GPU access\n",
    "print(f\"TensorFlow has access to the following devices:\\n{tf.config.list_physical_devices()}\")\n",
    "\n",
    "# See TensorFlow version\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f23c749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 00:38:44.938150: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-31 00:38:44.938226: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/585744285.py:28: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/585744285.py:239: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 00:38:45.180949: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-31 00:38:45.182157: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-31 00:38:45.214829: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-03-31 00:38:45.625852: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 3s 7ms/sample - loss: 0.3073\n",
      "epoch: 2\n",
      "Train on 158 samples\n",
      " 20/158 [==>...........................] - ETA: 1s - loss: 0.2389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-31 00:38:47.961848: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.2522\n",
      "epoch: 3\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.2182\n",
      "epoch: 4\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.2249\n",
      "epoch: 5\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1870\n",
      "epoch: 6\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1749\n",
      "epoch: 7\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1731\n",
      "epoch: 8\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1585\n",
      "epoch: 9\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1533\n",
      "epoch: 10\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1556\n",
      "epoch: 11\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1475\n",
      "epoch: 12\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1431\n",
      "epoch: 13\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1383\n",
      "epoch: 14\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1387\n",
      "epoch: 15\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1343\n",
      "epoch: 16\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1334\n",
      "epoch: 17\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1359\n",
      "epoch: 18\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1337\n",
      "epoch: 19\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1307\n",
      "epoch: 20\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1327\n",
      "epoch: 21\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1295\n",
      "epoch: 22\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1327\n",
      "epoch: 23\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1316\n",
      "epoch: 24\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1308\n",
      "epoch: 25\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1300\n",
      "epoch: 26\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1307\n",
      "epoch: 27\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1301\n",
      "epoch: 28\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1295\n",
      "epoch: 29\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1298\n",
      "epoch: 30\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1296\n",
      "epoch: 31\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1296\n",
      "epoch: 32\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1294\n",
      "epoch: 33\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1293\n",
      "epoch: 34\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1292\n",
      "epoch: 35\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1292\n",
      "epoch: 36\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1290\n",
      "epoch: 37\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1288\n",
      "epoch: 38\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1295\n",
      "epoch: 39\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1272\n",
      "epoch: 40\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1276\n",
      "epoch: 41\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1244\n",
      "epoch: 42\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1223\n",
      "epoch: 43\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1173\n",
      "epoch: 44\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1152\n",
      "epoch: 45\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1073\n",
      "epoch: 46\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1114\n",
      "epoch: 47\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1097\n",
      "epoch: 48\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1043\n",
      "epoch: 49\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 50\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1038\n",
      "epoch: 51\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0972\n",
      "epoch: 52\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1036\n",
      "epoch: 53\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 54\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1034\n",
      "epoch: 55\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 56\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 57\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1130\n",
      "epoch: 58\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0931\n",
      "epoch: 59\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 60\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 61\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1157\n",
      "epoch: 62\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1060\n",
      "epoch: 63\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 64\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 65\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0934\n",
      "epoch: 66\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0971\n",
      "epoch: 67\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 68\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 69\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 70\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 71\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1156\n",
      "epoch: 72\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 73\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 74\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 75\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 76\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 77\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1046\n",
      "epoch: 78\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 79\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 80\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 81\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1061\n",
      "epoch: 82\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0957\n",
      "epoch: 83\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1038\n",
      "epoch: 84\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 85\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 86\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1014\n",
      "epoch: 87\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 88\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 89\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 90\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 91\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1108\n",
      "epoch: 92\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0956\n",
      "epoch: 93\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1031\n",
      "epoch: 94\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 95\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1007\n",
      "epoch: 96\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0924\n",
      "epoch: 97\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 98\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 99\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 100\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0963\n",
      "epoch: 101\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 102\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1084\n",
      "epoch: 103\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 104\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 105\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 106\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1044\n",
      "epoch: 107\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 108\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1030\n",
      "epoch: 109\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0999\n",
      "epoch: 110\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 111\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 112\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0992\n",
      "epoch: 113\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1081\n",
      "epoch: 114\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 115\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1048\n",
      "epoch: 116\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 117\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1071\n",
      "epoch: 118\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1034\n",
      "epoch: 119\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0939\n",
      "epoch: 120\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0883\n",
      "epoch: 121\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 122\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 123\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0909\n",
      "epoch: 124\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0908\n",
      "epoch: 125\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 126\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 127\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1076\n",
      "epoch: 128\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 129\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 130\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0902\n",
      "epoch: 131\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0956\n",
      "epoch: 132\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 133\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 134\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 135\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1003\n",
      "epoch: 136\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 137\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1036\n",
      "epoch: 138\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 139\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 140\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 141\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0981\n",
      "epoch: 142\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 143\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0971\n",
      "epoch: 144\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 145\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 146\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1012\n",
      "epoch: 147\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1045\n",
      "epoch: 148\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1001\n",
      "epoch: 149\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1037\n",
      "epoch: 150\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0969\n",
      "epoch: 151\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 152\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0957\n",
      "epoch: 153\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0967\n",
      "epoch: 154\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 155\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1058\n",
      "epoch: 156\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 157\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 158\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1086\n",
      "epoch: 159\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1044\n",
      "epoch: 160\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 161\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 162\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1075\n",
      "epoch: 163\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 164\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1046\n",
      "epoch: 165\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 166\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 167\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0961\n",
      "epoch: 168\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0984\n",
      "epoch: 169\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0979\n",
      "epoch: 170\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0982\n",
      "epoch: 171\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0920\n",
      "epoch: 172\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 173\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1010\n",
      "epoch: 174\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 175\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1092\n",
      "epoch: 176\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0969\n",
      "epoch: 177\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 178\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 179\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1037\n",
      "epoch: 180\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 181\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0982\n",
      "epoch: 182\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 183\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 184\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0982\n",
      "epoch: 185\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 186\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0898\n",
      "epoch: 187\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0936\n",
      "epoch: 188\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 189\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 190\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0910\n",
      "epoch: 191\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1004\n",
      "epoch: 192\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 193\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0985\n",
      "epoch: 194\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0931\n",
      "epoch: 195\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0930\n",
      "epoch: 196\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 197\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0999\n",
      "epoch: 198\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 199\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 200\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 201\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 202\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0916\n",
      "epoch: 203\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0996\n",
      "epoch: 204\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0945\n",
      "epoch: 205\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1032\n",
      "epoch: 206\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 207\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0973\n",
      "epoch: 208\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0987\n",
      "epoch: 209\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 210\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1032\n",
      "epoch: 211\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0987\n",
      "epoch: 212\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0976\n",
      "epoch: 213\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1010\n",
      "epoch: 214\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0965\n",
      "epoch: 215\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0903\n",
      "epoch: 216\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0956\n",
      "epoch: 217\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 218\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1029\n",
      "epoch: 219\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0926\n",
      "epoch: 220\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1003\n",
      "epoch: 221\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0987\n",
      "epoch: 222\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0937\n",
      "epoch: 223\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0913\n",
      "epoch: 224\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1023\n",
      "epoch: 225\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1055\n",
      "epoch: 226\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1052\n",
      "epoch: 227\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1065\n",
      "epoch: 228\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1067\n",
      "epoch: 229\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1039\n",
      "epoch: 230\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1061\n",
      "epoch: 231\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 232\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 233\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 234\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1067\n",
      "epoch: 235\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1071\n",
      "epoch: 236\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1054\n",
      "epoch: 237\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0908\n",
      "epoch: 238\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 239\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 240\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1023\n",
      "epoch: 241\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 242\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0881\n",
      "epoch: 243\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0923\n",
      "epoch: 244\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1064\n",
      "epoch: 245\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0919\n",
      "epoch: 246\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0968\n",
      "epoch: 247\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0934\n",
      "epoch: 248\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 249\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1023\n",
      "epoch: 250\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1064\n",
      "epoch: 251\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0884\n",
      "epoch: 252\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1030\n",
      "epoch: 253\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1092\n",
      "epoch: 254\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0950\n",
      "epoch: 255\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0960\n",
      "epoch: 256\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1027\n",
      "epoch: 257\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0884\n",
      "epoch: 258\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 259\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0949\n",
      "epoch: 260\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 261\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 262\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 263\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 264\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 265\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0909\n",
      "epoch: 266\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 267\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0866\n",
      "epoch: 268\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0979\n",
      "epoch: 269\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 270\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1015\n",
      "epoch: 271\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0960\n",
      "epoch: 272\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1065\n",
      "epoch: 273\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 274\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1020\n",
      "epoch: 275\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0981\n",
      "epoch: 276\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 277\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 278\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1003\n",
      "epoch: 279\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0937\n",
      "epoch: 280\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0949\n",
      "epoch: 281\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1034\n",
      "epoch: 282\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 283\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 284\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1049\n",
      "epoch: 285\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1087\n",
      "epoch: 286\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 287\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1069\n",
      "epoch: 288\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 289\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 290\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1072\n",
      "epoch: 291\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0899\n",
      "epoch: 292\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 293\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1073\n",
      "epoch: 294\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1081\n",
      "epoch: 295\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0961\n",
      "epoch: 296\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 297\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1028\n",
      "epoch: 298\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1087\n",
      "epoch: 299\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0920\n",
      "epoch: 300\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0950\n",
      "epoch: 301\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1023\n",
      "epoch: 302\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1086\n",
      "epoch: 303\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 304\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 305\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 306\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0888\n",
      "epoch: 307\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 308\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 309\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 310\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1008\n",
      "epoch: 311\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0887\n",
      "epoch: 312\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0973\n",
      "epoch: 313\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0965\n",
      "epoch: 314\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0968\n",
      "epoch: 315\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1002\n",
      "epoch: 316\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0937\n",
      "epoch: 317\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 318\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 319\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0931\n",
      "epoch: 320\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1002\n",
      "epoch: 321\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 322\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1028\n",
      "epoch: 323\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 324\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 325\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 326\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0984\n",
      "epoch: 327\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0897\n",
      "epoch: 328\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 329\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1007\n",
      "epoch: 330\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0853\n",
      "epoch: 331\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 332\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 333\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1018\n",
      "epoch: 334\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0913\n",
      "epoch: 335\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 336\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 337\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0982\n",
      "epoch: 338\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 339\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0961\n",
      "epoch: 340\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0925\n",
      "epoch: 341\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0963\n",
      "epoch: 342\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 343\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 344\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0914\n",
      "epoch: 345\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0865\n",
      "epoch: 346\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0969\n",
      "epoch: 347\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0888\n",
      "epoch: 348\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 349\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0962\n",
      "epoch: 350\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 351\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 352\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1023\n",
      "epoch: 353\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 354\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1023\n",
      "epoch: 355\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 356\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 357\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1012\n",
      "epoch: 358\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 359\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 360\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1021\n",
      "epoch: 361\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 362\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 363\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1056\n",
      "epoch: 364\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 365\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1012\n",
      "epoch: 366\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0872\n",
      "epoch: 367\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 368\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1034\n",
      "epoch: 369\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1001\n",
      "epoch: 370\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1093\n",
      "epoch: 371\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0863\n",
      "epoch: 372\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1044\n",
      "epoch: 373\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1051\n",
      "epoch: 374\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0992\n",
      "epoch: 375\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1059\n",
      "epoch: 376\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 377\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 378\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0942\n",
      "epoch: 379\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 380\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1032\n",
      "epoch: 381\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0921\n",
      "epoch: 382\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0905\n",
      "epoch: 383\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1075\n",
      "epoch: 384\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0923\n",
      "epoch: 385\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0916\n",
      "epoch: 386\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0937\n",
      "epoch: 387\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1041\n",
      "epoch: 388\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 389\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 390\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 391\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 392\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 393\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 394\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0910\n",
      "epoch: 395\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 396\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0967\n",
      "epoch: 397\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 398\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 399\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1058\n",
      "epoch: 400\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1058\n",
      "epoch: 401\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0901\n",
      "epoch: 402\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0999\n",
      "epoch: 403\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 404\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 405\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0961\n",
      "epoch: 406\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0981\n",
      "epoch: 407\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0923\n",
      "epoch: 408\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 409\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 410\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1059\n",
      "epoch: 411\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 412\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 413\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1058\n",
      "epoch: 414\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 415\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 416\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 417\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0971\n",
      "epoch: 418\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1021\n",
      "epoch: 419\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0992\n",
      "epoch: 420\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1008\n",
      "epoch: 421\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0925\n",
      "epoch: 422\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0942\n",
      "epoch: 423\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 424\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 425\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 426\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0920\n",
      "epoch: 427\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0936\n",
      "epoch: 428\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 429\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 430\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 431\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0902\n",
      "epoch: 432\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0894\n",
      "epoch: 433\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 434\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 435\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0948\n",
      "epoch: 436\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0884\n",
      "epoch: 437\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0878\n",
      "epoch: 438\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1055\n",
      "epoch: 439\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0933\n",
      "epoch: 440\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0906\n",
      "epoch: 441\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0915\n",
      "epoch: 442\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0873\n",
      "epoch: 443\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1062\n",
      "epoch: 444\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 445\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 446\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0933\n",
      "epoch: 447\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 448\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 449\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0900\n",
      "epoch: 450\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0909\n",
      "epoch: 451\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0973\n",
      "epoch: 452\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 453\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 454\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 455\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 456\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 457\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 458\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0934\n",
      "epoch: 459\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1062\n",
      "epoch: 460\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1019\n",
      "epoch: 461\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 462\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1003\n",
      "epoch: 463\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1065\n",
      "epoch: 464\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0931\n",
      "epoch: 465\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0871\n",
      "epoch: 466\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0897\n",
      "epoch: 467\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1030\n",
      "epoch: 468\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1047\n",
      "epoch: 469\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1028\n",
      "epoch: 470\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1041\n",
      "epoch: 471\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0933\n",
      "epoch: 472\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0914\n",
      "epoch: 473\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0902\n",
      "epoch: 474\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0963\n",
      "epoch: 475\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 476\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1088\n",
      "epoch: 477\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1003\n",
      "epoch: 478\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1059\n",
      "epoch: 479\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 480\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 481\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1037\n",
      "epoch: 482\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 483\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0932\n",
      "epoch: 484\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 485\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0885\n",
      "epoch: 486\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1076\n",
      "epoch: 487\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 488\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 489\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0963\n",
      "epoch: 490\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1080\n",
      "epoch: 491\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 492\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1004\n",
      "epoch: 493\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0895\n",
      "epoch: 494\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 495\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 496\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0948\n",
      "epoch: 497\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 498\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0950\n",
      "epoch: 499\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0987\n",
      "epoch: 500\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1047\n",
      "epoch: 501\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 502\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 503\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0928\n",
      "epoch: 504\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 505\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0946\n",
      "epoch: 506\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0962\n",
      "epoch: 507\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 508\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1111\n",
      "epoch: 509\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1084\n",
      "epoch: 510\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0905\n",
      "epoch: 511\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 512\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 513\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0901\n",
      "epoch: 514\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0938\n",
      "epoch: 515\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 516\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1067\n",
      "epoch: 517\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 518\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0928\n",
      "epoch: 519\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 520\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1060\n",
      "epoch: 521\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 522\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 523\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1031\n",
      "epoch: 524\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0928\n",
      "epoch: 525\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1054\n",
      "epoch: 526\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0984\n",
      "epoch: 527\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1053\n",
      "epoch: 528\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 529\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1067\n",
      "epoch: 530\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 531\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 532\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0991\n",
      "epoch: 533\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1019\n",
      "epoch: 534\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 535\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1113\n",
      "epoch: 536\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 537\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1002\n",
      "epoch: 538\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0968\n",
      "epoch: 539\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 540\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0934\n",
      "epoch: 541\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1015\n",
      "epoch: 542\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 543\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1067\n",
      "epoch: 544\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1043\n",
      "epoch: 545\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 546\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0876\n",
      "epoch: 547\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 548\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0987\n",
      "epoch: 549\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1027\n",
      "epoch: 550\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1069\n",
      "epoch: 551\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1008\n",
      "epoch: 552\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1052\n",
      "epoch: 553\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 554\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1055\n",
      "epoch: 555\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 556\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1106\n",
      "epoch: 557\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0931\n",
      "epoch: 558\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0902\n",
      "epoch: 559\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0945\n",
      "epoch: 560\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 561\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1030\n",
      "epoch: 562\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 563\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 564\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0879\n",
      "epoch: 565\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1046\n",
      "epoch: 566\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0950\n",
      "epoch: 567\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 568\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0992\n",
      "epoch: 569\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0868\n",
      "epoch: 570\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 571\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1017\n",
      "epoch: 572\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1119\n",
      "epoch: 573\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0900\n",
      "epoch: 574\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0916\n",
      "epoch: 575\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0883\n",
      "epoch: 576\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 577\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1051\n",
      "epoch: 578\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1098\n",
      "epoch: 579\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 580\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0870\n",
      "epoch: 581\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0883\n",
      "epoch: 582\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0963\n",
      "epoch: 583\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0984\n",
      "epoch: 584\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0864\n",
      "epoch: 585\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0928\n",
      "epoch: 586\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 587\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 588\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 589\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0973\n",
      "epoch: 590\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1063\n",
      "epoch: 591\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1071\n",
      "epoch: 592\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0939\n",
      "epoch: 593\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 594\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0981\n",
      "epoch: 595\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0922\n",
      "epoch: 596\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 597\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1062\n",
      "epoch: 598\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0992\n",
      "epoch: 599\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1046\n",
      "epoch: 600\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0979\n",
      "epoch: 601\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0982\n",
      "epoch: 602\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 603\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0945\n",
      "epoch: 604\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1033\n",
      "epoch: 605\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 606\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1124\n",
      "epoch: 607\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1042\n",
      "epoch: 608\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1032\n",
      "epoch: 609\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1014\n",
      "epoch: 610\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 611\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0954\n",
      "epoch: 612\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 613\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 614\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1067\n",
      "epoch: 615\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0949\n",
      "epoch: 616\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1046\n",
      "epoch: 617\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0918\n",
      "epoch: 618\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0939\n",
      "epoch: 619\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0965\n",
      "epoch: 620\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1078\n",
      "epoch: 621\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0991\n",
      "epoch: 622\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0905\n",
      "epoch: 623\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1044\n",
      "epoch: 624\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 625\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 626\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0948\n",
      "epoch: 627\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 628\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0887\n",
      "epoch: 629\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1036\n",
      "epoch: 630\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0938\n",
      "epoch: 631\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 632\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1080\n",
      "epoch: 633\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0935\n",
      "epoch: 634\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1048\n",
      "epoch: 635\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 636\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1056\n",
      "epoch: 637\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 638\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 639\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 640\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 641\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0900\n",
      "epoch: 642\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0981\n",
      "epoch: 643\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 644\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0925\n",
      "epoch: 645\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 646\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 647\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1055\n",
      "epoch: 648\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1008\n",
      "epoch: 649\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1066\n",
      "epoch: 650\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0939\n",
      "epoch: 651\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 652\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0985\n",
      "epoch: 653\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1055\n",
      "epoch: 654\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0925\n",
      "epoch: 655\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0999\n",
      "epoch: 656\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1064\n",
      "epoch: 657\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1018\n",
      "epoch: 658\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0979\n",
      "epoch: 659\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 660\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1021\n",
      "epoch: 661\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1089\n",
      "epoch: 662\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1033\n",
      "epoch: 663\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0930\n",
      "epoch: 664\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0934\n",
      "epoch: 665\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 666\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0949\n",
      "epoch: 667\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 668\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0961\n",
      "epoch: 669\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 670\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1085\n",
      "epoch: 671\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0965\n",
      "epoch: 672\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 673\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1051\n",
      "epoch: 674\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0957\n",
      "epoch: 675\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 676\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1051\n",
      "epoch: 677\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1011\n",
      "epoch: 678\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 679\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 680\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 681\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1065\n",
      "epoch: 682\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0941\n",
      "epoch: 683\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1015\n",
      "epoch: 684\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0990\n",
      "epoch: 685\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 686\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0953\n",
      "epoch: 687\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 688\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0913\n",
      "epoch: 689\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0938\n",
      "epoch: 690\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0971\n",
      "epoch: 691\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0887\n",
      "epoch: 692\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 693\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 694\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0999\n",
      "epoch: 695\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 696\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1026\n",
      "epoch: 697\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1049\n",
      "epoch: 698\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 699\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1034\n",
      "epoch: 700\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0880\n",
      "epoch: 701\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1084\n",
      "epoch: 702\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1033\n",
      "epoch: 703\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0930\n",
      "epoch: 704\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0943\n",
      "epoch: 705\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1046\n",
      "epoch: 706\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 707\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0937\n",
      "epoch: 708\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0925\n",
      "epoch: 709\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 710\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1019\n",
      "epoch: 711\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1043\n",
      "epoch: 712\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1110\n",
      "epoch: 713\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1060\n",
      "epoch: 714\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 715\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0872\n",
      "epoch: 716\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0944\n",
      "epoch: 717\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0975\n",
      "epoch: 718\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0920\n",
      "epoch: 719\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0972\n",
      "epoch: 720\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1040\n",
      "epoch: 721\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1109\n",
      "epoch: 722\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0996\n",
      "epoch: 723\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 724\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1021\n",
      "epoch: 725\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0912\n",
      "epoch: 726\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1039\n",
      "epoch: 727\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1061\n",
      "epoch: 728\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1001\n",
      "epoch: 729\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0961\n",
      "epoch: 730\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0987\n",
      "epoch: 731\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1064\n",
      "epoch: 732\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0947\n",
      "epoch: 733\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1016\n",
      "epoch: 734\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0848\n",
      "epoch: 735\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1071\n",
      "epoch: 736\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0926\n",
      "epoch: 737\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0974\n",
      "epoch: 738\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1101\n",
      "epoch: 739\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0993\n",
      "epoch: 740\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1012\n",
      "epoch: 741\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0918\n",
      "epoch: 742\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0972\n",
      "epoch: 743\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0880\n",
      "epoch: 744\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0943\n",
      "epoch: 745\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0986\n",
      "epoch: 746\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0980\n",
      "epoch: 747\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0949\n",
      "epoch: 748\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1020\n",
      "epoch: 749\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0947\n",
      "epoch: 750\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0872\n",
      "epoch: 751\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1058\n",
      "epoch: 752\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0976\n",
      "epoch: 753\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0942\n",
      "epoch: 754\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0976\n",
      "epoch: 755\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1010\n",
      "epoch: 756\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0962\n",
      "epoch: 757\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1016\n",
      "epoch: 758\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0929\n",
      "epoch: 759\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0958\n",
      "epoch: 760\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0993\n",
      "epoch: 761\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0960\n",
      "epoch: 762\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0970\n",
      "epoch: 763\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0895\n",
      "epoch: 764\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0936\n",
      "epoch: 765\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1001\n",
      "epoch: 766\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 767\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0938\n",
      "epoch: 768\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0939\n",
      "epoch: 769\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1037\n",
      "epoch: 770\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1122\n",
      "epoch: 771\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0847\n",
      "epoch: 772\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 773\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 774\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1036\n",
      "epoch: 775\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0996\n",
      "epoch: 776\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1032\n",
      "epoch: 777\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0983\n",
      "epoch: 778\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1065\n",
      "epoch: 779\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0997\n",
      "epoch: 780\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0993\n",
      "epoch: 781\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 782\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0882\n",
      "epoch: 783\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0872\n",
      "epoch: 784\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1002\n",
      "epoch: 785\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0891\n",
      "epoch: 786\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0932\n",
      "epoch: 787\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0962\n",
      "epoch: 788\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0948\n",
      "epoch: 789\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0915\n",
      "epoch: 790\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0903\n",
      "epoch: 791\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0964\n",
      "epoch: 792\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 793\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1056\n",
      "epoch: 794\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0967\n",
      "epoch: 795\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1024\n",
      "epoch: 796\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0930\n",
      "epoch: 797\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0922\n",
      "epoch: 798\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1010\n",
      "epoch: 799\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1021\n",
      "epoch: 800\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1042\n",
      "epoch: 801\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0964\n",
      "epoch: 802\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1011\n",
      "epoch: 803\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0956\n",
      "epoch: 804\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0975\n",
      "epoch: 805\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1007\n",
      "epoch: 806\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1000\n",
      "epoch: 807\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0887\n",
      "epoch: 808\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0989\n",
      "epoch: 809\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1068\n",
      "epoch: 810\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0981\n",
      "epoch: 811\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1014\n",
      "epoch: 812\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1012\n",
      "epoch: 813\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1114\n",
      "epoch: 814\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1018\n",
      "epoch: 815\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1052\n",
      "epoch: 816\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1054\n",
      "epoch: 817\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0970\n",
      "epoch: 818\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0990\n",
      "epoch: 819\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0965\n",
      "epoch: 820\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0976\n",
      "epoch: 821\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1017\n",
      "epoch: 822\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1065\n",
      "epoch: 823\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0954\n",
      "epoch: 824\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1006\n",
      "epoch: 825\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 826\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1051\n",
      "epoch: 827\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0953\n",
      "epoch: 828\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0965\n",
      "epoch: 829\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0999\n",
      "epoch: 830\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 831\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 832\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 833\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1043\n",
      "epoch: 834\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1066\n",
      "epoch: 835\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0960\n",
      "epoch: 836\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0977\n",
      "epoch: 837\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0900\n",
      "epoch: 838\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0966\n",
      "epoch: 839\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0985\n",
      "epoch: 840\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1021\n",
      "epoch: 841\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0937\n",
      "epoch: 842\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0905\n",
      "epoch: 843\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 844\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 845\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1020\n",
      "epoch: 846\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 847\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 848\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0978\n",
      "epoch: 849\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.1025\n",
      "epoch: 850\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0974\n",
      "epoch: 851\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0911\n",
      "epoch: 852\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0952\n",
      "epoch: 853\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0968\n",
      "epoch: 854\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0875\n",
      "epoch: 855\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1007\n",
      "epoch: 856\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1056\n",
      "epoch: 857\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 858\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1016\n",
      "epoch: 859\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0983\n",
      "epoch: 860\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0901\n",
      "epoch: 861\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1037\n",
      "epoch: 862\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0922\n",
      "epoch: 863\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.0997\n",
      "epoch: 864\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 865\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0912\n",
      "epoch: 866\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0940\n",
      "epoch: 867\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0958\n",
      "epoch: 868\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0898\n",
      "epoch: 869\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1105\n",
      "epoch: 870\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 871\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0949\n",
      "epoch: 872\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1015\n",
      "epoch: 873\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 874\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1052\n",
      "epoch: 875\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0939\n",
      "epoch: 876\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 877\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 878\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0921\n",
      "epoch: 879\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1087\n",
      "epoch: 880\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0955\n",
      "epoch: 881\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0976\n",
      "epoch: 882\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0969\n",
      "epoch: 883\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 884\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0908\n",
      "epoch: 885\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1029\n",
      "epoch: 886\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0994\n",
      "epoch: 887\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1024\n",
      "epoch: 888\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0971\n",
      "epoch: 889\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 890\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0993\n",
      "epoch: 891\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1057\n",
      "epoch: 892\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 893\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 894\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0951\n",
      "epoch: 895\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0947\n",
      "epoch: 896\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0870\n",
      "epoch: 897\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0930\n",
      "epoch: 898\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1055\n",
      "epoch: 899\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0956\n",
      "epoch: 900\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 901\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1081\n",
      "epoch: 902\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0875\n",
      "epoch: 903\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0988\n",
      "epoch: 904\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1005\n",
      "epoch: 905\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1063\n",
      "epoch: 906\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0924\n",
      "epoch: 907\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1008\n",
      "epoch: 908\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1025\n",
      "epoch: 909\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0978\n",
      "epoch: 910\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0919\n",
      "epoch: 911\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1009\n",
      "epoch: 912\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1022\n",
      "epoch: 913\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0998\n",
      "epoch: 914\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0995\n",
      "epoch: 915\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0933\n",
      "epoch: 916\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 917\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0989\n",
      "epoch: 918\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0907\n",
      "epoch: 919\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1025\n",
      "epoch: 920\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1013\n",
      "epoch: 921\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0986\n",
      "epoch: 922\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1056\n",
      "epoch: 923\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1004\n",
      "epoch: 924\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1050\n",
      "epoch: 925\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1006\n",
      "epoch: 926\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1035\n",
      "epoch: 927\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1038\n",
      "epoch: 928\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0942\n",
      "epoch: 929\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0968\n",
      "epoch: 930\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0917\n",
      "epoch: 931\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0991\n",
      "epoch: 932\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0949\n",
      "epoch: 933\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 6ms/sample - loss: 0.0980\n",
      "epoch: 934\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0967\n",
      "epoch: 935\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0959\n",
      "epoch: 936\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 937\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0954\n",
      "epoch: 938\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0894\n",
      "epoch: 939\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0999\n",
      "epoch: 940\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1079\n",
      "epoch: 941\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1146\n",
      "epoch: 942\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0887\n",
      "epoch: 943\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1130\n",
      "epoch: 944\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0980\n",
      "epoch: 945\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.1000\n",
      "epoch: 946\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0997\n",
      "epoch: 947\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 948\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0973\n",
      "epoch: 949\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0938\n",
      "epoch: 950\n",
      "Train on 158 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0975\n",
      "epoch: 951\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0944\n",
      "epoch: 952\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 7ms/sample - loss: 0.0996\n",
      "epoch: 953\n",
      "Train on 158 samples\n",
      "158/158 [==============================] - 1s 8ms/sample - loss: 0.1014\n",
      "Forecasting Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-03-31 00:56:00.232242: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=10.298886, Expected=9.510000\n",
      "Month=2, Predicted=9.838371, Expected=9.796000\n",
      "Month=3, Predicted=9.718878, Expected=9.468500\n",
      "Month=4, Predicted=9.616369, Expected=9.672000\n",
      "Month=5, Predicted=9.610226, Expected=9.610000\n",
      "Month=6, Predicted=9.636385, Expected=9.240000\n",
      "Month=7, Predicted=9.395954, Expected=10.318300\n",
      "Month=8, Predicted=9.883541, Expected=8.974800\n",
      "Month=9, Predicted=9.478987, Expected=9.114000\n",
      "Month=10, Predicted=9.091464, Expected=9.300000\n",
      "Month=11, Predicted=9.234981, Expected=8.400000\n",
      "Month=12, Predicted=8.772859, Expected=9.300000\n",
      "Month=13, Predicted=8.969271, Expected=9.000000\n",
      "Month=14, Predicted=9.099602, Expected=9.300000\n",
      "Month=15, Predicted=9.167929, Expected=9.460000\n",
      "Month=16, Predicted=9.363471, Expected=9.145000\n",
      "Month=17, Predicted=9.240445, Expected=9.021000\n",
      "Month=18, Predicted=9.061088, Expected=8.750000\n",
      "Month=19, Predicted=8.858772, Expected=8.710000\n",
      "Month=20, Predicted=8.735935, Expected=8.370000\n",
      "Month=21, Predicted=8.518534, Expected=8.504000\n",
      "Month=22, Predicted=8.468953, Expected=9.819700\n",
      "Month=23, Predicted=9.269205, Expected=9.827300\n",
      "Month=24, Predicted=9.733773, Expected=9.929800\n",
      "Month=25, Predicted=9.811227, Expected=9.288000\n",
      "Month=26, Predicted=9.489185, Expected=9.300000\n",
      "Month=27, Predicted=9.279784, Expected=9.060000\n",
      "Month=28, Predicted=9.144385, Expected=8.835000\n",
      "Month=29, Predicted=8.928492, Expected=8.388600\n",
      "Month=30, Predicted=8.586291, Expected=8.400000\n",
      "Month=31, Predicted=8.426914, Expected=8.525000\n",
      "Month=32, Predicted=8.490780, Expected=8.250000\n",
      "Month=33, Predicted=8.367359, Expected=8.419000\n",
      "Month=34, Predicted=8.362076, Expected=9.455000\n",
      "Month=35, Predicted=9.013644, Expected=8.540000\n",
      "Month=36, Predicted=8.848266, Expected=9.455000\n",
      "Month=37, Predicted=9.066033, Expected=9.000000\n",
      "Month=38, Predicted=9.123628, Expected=9.599000\n",
      "Month=39, Predicted=9.319515, Expected=9.436000\n",
      "Month=40, Predicted=9.439124, Expected=9.539800\n",
      "Month=41, Predicted=9.451653, Expected=9.028600\n",
      "Month=42, Predicted=9.198806, Expected=8.932000\n",
      "Month=43, Predicted=8.967433, Expected=8.993000\n",
      "Month=44, Predicted=8.964904, Expected=8.678400\n",
      "Month=45, Predicted=8.800829, Expected=9.011100\n",
      "Month=46, Predicted=8.878834, Expected=9.630000\n",
      "Month=47, Predicted=9.348159, Expected=8.590400\n",
      "Month=48, Predicted=8.966609, Expected=9.736300\n",
      "Month=49, Predicted=9.270313, Expected=9.384500\n",
      "Month=50, Predicted=9.464758, Expected=9.947200\n",
      "Month=51, Predicted=9.676216, Expected=9.577100\n",
      "Month=52, Predicted=9.664877, Expected=9.117200\n",
      "Month=53, Predicted=9.279115, Expected=9.122500\n",
      "Month=54, Predicted=9.121625, Expected=8.880000\n",
      "Month=55, Predicted=8.978507, Expected=8.709200\n",
      "Month=56, Predicted=8.789869, Expected=8.428200\n",
      "Month=57, Predicted=8.560085, Expected=9.907600\n",
      "Month=58, Predicted=9.309817, Expected=9.145000\n",
      "Month=59, Predicted=9.382784, Expected=8.498000\n",
      "Month=60, Predicted=8.753128, Expected=9.362000\n",
      "Month=61, Predicted=9.021928, Expected=9.000000\n",
      "Month=62, Predicted=9.109231, Expected=9.455000\n",
      "Month=63, Predicted=9.249021, Expected=9.300000\n",
      "Month=64, Predicted=9.319534, Expected=8.990000\n",
      "Month=65, Predicted=9.091523, Expected=8.990000\n",
      "Month=66, Predicted=8.983599, Expected=8.790000\n",
      "Month=67, Predicted=8.864980, Expected=8.835000\n",
      "Month=68, Predicted=8.817413, Expected=8.700000\n",
      "Month=69, Predicted=8.750474, Expected=8.935000\n",
      "Month=70, Predicted=8.835039, Expected=8.835000\n",
      "Month=71, Predicted=8.856318, Expected=8.265000\n",
      "Month=72, Predicted=8.491793, Expected=8.835000\n",
      "Month=73, Predicted=8.616787, Expected=8.550000\n",
      "Month=74, Predicted=8.644821, Expected=8.680000\n",
      "Month=75, Predicted=8.619930, Expected=8.400000\n",
      "Month=76, Predicted=8.501021, Expected=8.525000\n",
      "Month=77, Predicted=8.473035, Expected=8.370000\n",
      "Month=78, Predicted=8.423126, Expected=7.890000\n",
      "Month=79, Predicted=8.089039, Expected=7.812000\n",
      "Month=80, Predicted=7.869283, Expected=7.620000\n",
      "Month=81, Predicted=7.720080, Expected=7.718000\n",
      "Month=82, Predicted=7.698997, Expected=8.323500\n",
      "Month=83, Predicted=8.073674, Expected=6.860000\n",
      "Month=84, Predicted=7.434711, Expected=8.308000\n",
      "Month=85, Predicted=7.757565, Expected=8.100000\n",
      "Month=86, Predicted=8.132596, Expected=8.525000\n",
      "Month=87, Predicted=8.312420, Expected=8.250000\n",
      "Month=88, Predicted=8.307249, Expected=8.215000\n",
      "Month=89, Predicted=8.198397, Expected=8.122600\n",
      "Month=90, Predicted=8.135282, Expected=7.778100\n",
      "Month=91, Predicted=7.905053, Expected=7.954600\n",
      "Month=92, Predicted=7.884162, Expected=7.420000\n",
      "Month=93, Predicted=7.632494, Expected=7.538300\n",
      "Month=94, Predicted=7.508989, Expected=7.905000\n",
      "Month=95, Predicted=7.753237, Expected=7.140000\n",
      "Month=96, Predicted=7.436074, Expected=8.432000\n",
      "Month=97, Predicted=7.913775, Expected=7.710000\n",
      "Month=98, Predicted=7.943934, Expected=7.967000\n",
      "Month=99, Predicted=7.847774, Expected=7.320000\n",
      "Month=100, Predicted=7.563140, Expected=7.502000\n",
      "Month=101, Predicted=7.440505, Expected=7.409000\n",
      "Month=102, Predicted=7.443220, Expected=7.200600\n",
      "Month=103, Predicted=7.286359, Expected=7.865000\n",
      "Month=104, Predicted=7.592003, Expected=6.690000\n",
      "Month=105, Predicted=7.141643, Expected=6.879400\n",
      "Month=106, Predicted=6.838534, Expected=7.440000\n",
      "Month=107, Predicted=7.215691, Expected=6.860000\n",
      "Month=108, Predicted=7.072246, Expected=7.595000\n",
      "Month=109, Predicted=7.293841, Expected=7.200000\n",
      "Month=110, Predicted=7.319842, Expected=7.130000\n",
      "Month=111, Predicted=7.145436, Expected=6.900000\n",
      "Month=112, Predicted=6.986433, Expected=7.130000\n",
      "Month=113, Predicted=7.035303, Expected=7.130000\n",
      "Month=114, Predicted=7.111808, Expected=6.840000\n",
      "Month=115, Predicted=6.943980, Expected=7.006000\n",
      "Month=116, Predicted=6.936413, Expected=6.780000\n",
      "Month=117, Predicted=6.860150, Expected=7.089600\n",
      "Month=118, Predicted=6.957527, Expected=6.882000\n",
      "Month=119, Predicted=6.943304, Expected=6.446700\n",
      "Month=120, Predicted=6.619686, Expected=6.882000\n",
      "Month=121, Predicted=6.714072, Expected=6.600000\n",
      "Month=122, Predicted=6.697541, Expected=6.820000\n",
      "Month=123, Predicted=6.724739, Expected=6.600000\n",
      "Month=124, Predicted=6.671787, Expected=6.820000\n",
      "Month=125, Predicted=6.720935, Expected=6.665000\n",
      "Month=126, Predicted=6.706390, Expected=6.450000\n",
      "Month=127, Predicted=6.528178, Expected=6.665000\n",
      "Month=128, Predicted=6.574186, Expected=6.450000\n",
      "Month=129, Predicted=6.521804, Expected=6.722100\n",
      "Month=130, Predicted=6.602316, Expected=6.820000\n",
      "Month=131, Predicted=6.751980, Expected=6.160000\n",
      "Month=132, Predicted=6.407028, Expected=6.820000\n",
      "Month=133, Predicted=6.556429, Expected=6.480000\n",
      "Month=134, Predicted=6.586297, Expected=6.596900\n",
      "Month=135, Predicted=6.537069, Expected=6.492000\n",
      "Month=136, Predicted=6.515840, Expected=6.510000\n",
      "Month=137, Predicted=6.489107, Expected=6.339500\n",
      "Month=138, Predicted=6.395536, Expected=6.001600\n",
      "Month=139, Predicted=6.138918, Expected=6.107000\n",
      "Month=140, Predicted=6.076124, Expected=5.790000\n",
      "Month=141, Predicted=5.922653, Expected=5.885000\n",
      "Month=142, Predicted=5.860279, Expected=7.280000\n",
      "Month=143, Predicted=6.694480, Expected=5.941600\n",
      "Month=144, Predicted=6.408639, Expected=6.810000\n",
      "Month=145, Predicted=6.453408, Expected=6.182000\n",
      "Month=146, Predicted=6.391807, Expected=6.293000\n",
      "Month=147, Predicted=6.241065, Expected=6.118600\n",
      "Month=148, Predicted=6.176358, Expected=6.138000\n",
      "Month=149, Predicted=6.124650, Expected=6.107000\n",
      "Month=150, Predicted=6.110206, Expected=5.913000\n",
      "Month=151, Predicted=5.984855, Expected=6.141100\n",
      "Month=152, Predicted=6.045289, Expected=6.248000\n",
      "Month=153, Predicted=6.183286, Expected=5.829700\n",
      "Month=154, Predicted=5.979623, Expected=6.829300\n",
      "Month=155, Predicted=6.410875, Expected=6.694400\n",
      "Month=156, Predicted=6.681864, Expected=7.726200\n",
      "Month=157, Predicted=7.242838, Expected=7.054400\n",
      "Month=158, Predicted=7.225595, Expected=7.268900\n",
      "Train RMSE: 0.4377\n",
      "Train RMSPE: 5.4199\n",
      "Train MAE: 0.31869\n",
      "Train MAPE: 3.97222\n",
      "Forecasting Testing Data\n",
      "Month=1, Predicted=7.132212, Expected=7.020000\n",
      "Month=2, Predicted=7.070454, Expected=6.510000\n",
      "Month=3, Predicted=6.695577, Expected=6.370500\n",
      "Month=4, Predicted=6.436050, Expected=5.730000\n",
      "Month=5, Predicted=6.010034, Expected=5.828000\n",
      "Month=6, Predicted=5.831236, Expected=5.580000\n",
      "Month=7, Predicted=5.707408, Expected=5.709900\n",
      "Month=8, Predicted=5.683846, Expected=6.696000\n",
      "Month=9, Predicted=6.288928, Expected=6.248000\n",
      "Month=10, Predicted=6.373154, Expected=6.711600\n",
      "Month=11, Predicted=6.492134, Expected=6.600100\n",
      "Month=12, Predicted=6.592971, Expected=7.508200\n",
      "Month=13, Predicted=7.086783, Expected=7.765000\n",
      "Month=14, Predicted=7.560379, Expected=7.285000\n",
      "Month=15, Predicted=7.390615, Expected=6.959500\n",
      "Month=16, Predicted=7.049942, Expected=6.450000\n",
      "Month=17, Predicted=6.646390, Expected=6.572000\n",
      "Month=18, Predicted=6.535327, Expected=6.600000\n",
      "Month=19, Predicted=6.586681, Expected=4.265300\n",
      "Month=20, Predicted=5.229883, Expected=7.367000\n",
      "Month=21, Predicted=6.211959, Expected=6.544000\n",
      "Month=22, Predicted=6.796084, Expected=6.940800\n",
      "Month=23, Predicted=6.750558, Expected=6.786000\n",
      "Month=24, Predicted=6.802527, Expected=6.981200\n",
      "Month=25, Predicted=6.866958, Expected=6.756000\n",
      "Month=26, Predicted=6.808897, Expected=6.733200\n",
      "Month=27, Predicted=6.720940, Expected=6.671200\n",
      "Month=28, Predicted=6.677518, Expected=6.295600\n",
      "Month=29, Predicted=6.438612, Expected=6.432500\n",
      "Month=30, Predicted=6.383074, Expected=6.153000\n",
      "Month=31, Predicted=6.263719, Expected=6.389500\n",
      "Month=32, Predicted=6.297901, Expected=7.192000\n",
      "Month=33, Predicted=6.839405, Expected=6.524000\n",
      "Month=34, Predicted=6.736521, Expected=7.238500\n",
      "Month=35, Predicted=6.923854, Expected=6.990000\n",
      "Month=36, Predicted=7.032023, Expected=7.254000\n",
      "Month=37, Predicted=7.106161, Expected=6.720000\n",
      "Month=38, Predicted=6.895119, Expected=6.944000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=39, Predicted=6.841868, Expected=7.052500\n",
      "Month=40, Predicted=6.981193, Expected=6.690000\n",
      "Month=41, Predicted=6.811684, Expected=6.909900\n",
      "Month=42, Predicted=6.812541, Expected=6.819000\n",
      "Month=43, Predicted=6.834585, Expected=7.167200\n",
      "Month=44, Predicted=7.004085, Expected=7.254000\n",
      "Month=45, Predicted=7.176864, Expected=6.664000\n",
      "Month=46, Predicted=6.871589, Expected=7.393500\n",
      "Month=47, Predicted=7.088617, Expected=7.125000\n",
      "Month=48, Predicted=7.187053, Expected=7.347000\n",
      "Month=49, Predicted=7.227189, Expected=7.216500\n",
      "Month=50, Predicted=7.232016, Expected=7.254000\n",
      "Month=51, Predicted=7.211887, Expected=7.238500\n",
      "Month=52, Predicted=7.217726, Expected=6.990000\n",
      "Month=53, Predicted=7.070472, Expected=7.192000\n",
      "Month=54, Predicted=7.099885, Expected=6.900000\n",
      "Month=55, Predicted=6.999995, Expected=7.427300\n",
      "Month=56, Predicted=7.201654, Expected=7.300500\n",
      "Month=57, Predicted=7.311648, Expected=6.902000\n",
      "Month=58, Predicted=7.041721, Expected=7.409000\n",
      "Month=59, Predicted=7.195000, Expected=7.179000\n",
      "Month=60, Predicted=7.237530, Expected=7.424500\n",
      "Month=61, Predicted=7.301964, Expected=7.275000\n",
      "Month=62, Predicted=7.302534, Expected=7.316000\n",
      "Month=63, Predicted=7.276763, Expected=7.086300\n",
      "Month=64, Predicted=7.159178, Expected=7.020000\n",
      "Month=65, Predicted=7.040282, Expected=7.270500\n",
      "Month=66, Predicted=7.158520, Expected=7.168800\n",
      "Month=67, Predicted=7.185580, Expected=7.448600\n",
      "Month=68, Predicted=7.312507, Expected=7.440200\n",
      "Test RMSE: 0.5077\n",
      "Test RMSPE: 8.9235\n",
      "Test MAE: 0.30556\n",
      "Test MAPE: 4.72571\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(SimpleRNN(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,kernel_initializer='he_normal',recurrent_initializer='lecun_uniform'))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[3], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('epoch:',i+1)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)\n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)\n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.7)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')\n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.4f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.4f' % rmspe_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.4f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_test = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "\tprint('Test RMSPE: %.4f' % rmspe_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper left')\n",
    "\tax.set_xlabel('Month',fontsize = 16)\n",
    "\tax.set_ylabel('cumulative oil production',fontsize = 16 )\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True)\n",
    "\tlook_back= 2\n",
    "\tneurons=[1]\n",
    "\tn_epoch=953\n",
    "\texperiment(series,look_back,neurons,n_epoch)\n",
    "\n",
    "run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193f4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 11:58:41.872851: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 11:58:41.873596: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/4033664626.py:27: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/4033664626.py:236: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
      "2022-04-01 11:58:42.020418: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 11:58:42.020699: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "epoch: 1\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 11:58:42.426996: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 11:58:42.474908: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 11:58:42.829624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 7s 25ms/sample - loss: 0.0526\n",
      "epoch: 2\n",
      "Train on 180 samples\n",
      "  5/180 [..............................] - ETA: 4s - loss: 0.0272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 11:58:49.331285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 11:58:49.351338: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 11:58:49.370789: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0482\n",
      "epoch: 3\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0497\n",
      "epoch: 4\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0500\n",
      "epoch: 5\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0468\n",
      "epoch: 6\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0486\n",
      "epoch: 7\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0448\n",
      "epoch: 8\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0437\n",
      "epoch: 9\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0461\n",
      "epoch: 10\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0439\n",
      "epoch: 11\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0418\n",
      "epoch: 12\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0420\n",
      "epoch: 13\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0425\n",
      "epoch: 14\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0448\n",
      "epoch: 15\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0393\n",
      "epoch: 16\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0357\n",
      "epoch: 17\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0406\n",
      "epoch: 18\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0420\n",
      "epoch: 19\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0431\n",
      "epoch: 20\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0395\n",
      "epoch: 21\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0382\n",
      "epoch: 22\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0375\n",
      "epoch: 23\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0417\n",
      "epoch: 24\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0376\n",
      "epoch: 25\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0383\n",
      "epoch: 26\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0437\n",
      "epoch: 27\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0394\n",
      "epoch: 28\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0436\n",
      "epoch: 29\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0449\n",
      "epoch: 30\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0435\n",
      "epoch: 31\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0402\n",
      "epoch: 32\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0393\n",
      "epoch: 33\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0428\n",
      "epoch: 34\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0432\n",
      "epoch: 35\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0385\n",
      "epoch: 36\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0406\n",
      "epoch: 37\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 38\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0430\n",
      "epoch: 39\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0464\n",
      "epoch: 40\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0463\n",
      "epoch: 41\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 42\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0397\n",
      "epoch: 43\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0403\n",
      "epoch: 44\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0425\n",
      "epoch: 45\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0400\n",
      "epoch: 46\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0437\n",
      "epoch: 47\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0386\n",
      "epoch: 48\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0353\n",
      "epoch: 49\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0383\n",
      "epoch: 50\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0369\n",
      "epoch: 51\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0383\n",
      "epoch: 52\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0429\n",
      "epoch: 53\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0354\n",
      "epoch: 54\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0415\n",
      "epoch: 55\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0407\n",
      "epoch: 56\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0387\n",
      "epoch: 57\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0426\n",
      "epoch: 58\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0355\n",
      "epoch: 59\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0332\n",
      "epoch: 60\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0446\n",
      "epoch: 61\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0371\n",
      "epoch: 62\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0376\n",
      "epoch: 63\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0367\n",
      "epoch: 64\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0437\n",
      "epoch: 65\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0431\n",
      "epoch: 66\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0369\n",
      "epoch: 67\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0421\n",
      "epoch: 68\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0434\n",
      "epoch: 69\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0330\n",
      "epoch: 70\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0451\n",
      "epoch: 71\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0378\n",
      "epoch: 72\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0357\n",
      "epoch: 73\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0393\n",
      "epoch: 74\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0362\n",
      "epoch: 75\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0336\n",
      "epoch: 76\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0357\n",
      "epoch: 77\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0403\n",
      "epoch: 78\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0401\n",
      "epoch: 79\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0419\n",
      "epoch: 80\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0428\n",
      "epoch: 81\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0410\n",
      "epoch: 82\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0380\n",
      "epoch: 83\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0352\n",
      "epoch: 84\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0401\n",
      "epoch: 85\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0370\n",
      "epoch: 86\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0360\n",
      "epoch: 87\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0346\n",
      "epoch: 88\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0420\n",
      "epoch: 89\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0419\n",
      "epoch: 90\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0330\n",
      "epoch: 91\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0377\n",
      "epoch: 92\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0365\n",
      "epoch: 93\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0359\n",
      "epoch: 94\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0438\n",
      "epoch: 95\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0365\n",
      "epoch: 96\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0412\n",
      "epoch: 97\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0376\n",
      "epoch: 98\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0363\n",
      "epoch: 99\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0363\n",
      "epoch: 100\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0428\n",
      "epoch: 101\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0441\n",
      "epoch: 102\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0428\n",
      "epoch: 103\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0370\n",
      "epoch: 104\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0454\n",
      "epoch: 105\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0378\n",
      "epoch: 106\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0401\n",
      "epoch: 107\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0433\n",
      "epoch: 108\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0412\n",
      "epoch: 109\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0335\n",
      "epoch: 110\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0334\n",
      "epoch: 111\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0414\n",
      "epoch: 112\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0342\n",
      "epoch: 113\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0395\n",
      "epoch: 114\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0372\n",
      "epoch: 115\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0346\n",
      "epoch: 116\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0413\n",
      "epoch: 117\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0351\n",
      "epoch: 118\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0433\n",
      "epoch: 119\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0403\n",
      "epoch: 120\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0403\n",
      "epoch: 121\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0448\n",
      "epoch: 122\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0407\n",
      "epoch: 123\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0357\n",
      "epoch: 124\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0369\n",
      "epoch: 125\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0348\n",
      "epoch: 126\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0397\n",
      "epoch: 127\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0415\n",
      "epoch: 128\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0366\n",
      "epoch: 129\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0410\n",
      "epoch: 130\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0357\n",
      "epoch: 131\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0386\n",
      "epoch: 132\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0386\n",
      "epoch: 133\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0328\n",
      "epoch: 134\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0409\n",
      "epoch: 135\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0448\n",
      "epoch: 136\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0427\n",
      "epoch: 137\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0363\n",
      "epoch: 138\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0359\n",
      "epoch: 139\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0420\n",
      "epoch: 140\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0443\n",
      "epoch: 141\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0421\n",
      "epoch: 142\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0347\n",
      "epoch: 143\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0375\n",
      "epoch: 144\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 145\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0357\n",
      "epoch: 146\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0348\n",
      "epoch: 147\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0394\n",
      "epoch: 148\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0404\n",
      "epoch: 149\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0369\n",
      "epoch: 150\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0332\n",
      "epoch: 151\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0421\n",
      "epoch: 152\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0392\n",
      "epoch: 153\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0392\n",
      "epoch: 154\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0384\n",
      "epoch: 155\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0381\n",
      "epoch: 156\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0355\n",
      "epoch: 157\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0438\n",
      "epoch: 158\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0432\n",
      "epoch: 159\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0350\n",
      "epoch: 160\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0340\n",
      "epoch: 161\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0368\n",
      "epoch: 162\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0341\n",
      "epoch: 163\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0377\n",
      "epoch: 164\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0311\n",
      "epoch: 165\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0415\n",
      "epoch: 166\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0423\n",
      "epoch: 167\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0436\n",
      "epoch: 168\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0404\n",
      "epoch: 169\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0317\n",
      "epoch: 170\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0336\n",
      "epoch: 171\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0419\n",
      "epoch: 172\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0384\n",
      "epoch: 173\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0409\n",
      "epoch: 174\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0414\n",
      "epoch: 175\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0352\n",
      "epoch: 176\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0386\n",
      "epoch: 177\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0394\n",
      "epoch: 178\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0426\n",
      "epoch: 179\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0380\n",
      "epoch: 180\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0409\n",
      "epoch: 181\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0438\n",
      "epoch: 182\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0429\n",
      "epoch: 183\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0375\n",
      "epoch: 184\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0375\n",
      "epoch: 185\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0342\n",
      "epoch: 186\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0367\n",
      "epoch: 187\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0370\n",
      "epoch: 188\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0395\n",
      "epoch: 189\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0373\n",
      "epoch: 190\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0345\n",
      "epoch: 191\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0381\n",
      "epoch: 192\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0443\n",
      "epoch: 193\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0384\n",
      "epoch: 194\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0379\n",
      "epoch: 195\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0355\n",
      "epoch: 196\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0420\n",
      "epoch: 197\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0355\n",
      "epoch: 198\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0443\n",
      "epoch: 199\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0372\n",
      "epoch: 200\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0414\n",
      "epoch: 201\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0355\n",
      "epoch: 202\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0398\n",
      "epoch: 203\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0347\n",
      "epoch: 204\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0372\n",
      "epoch: 205\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0375\n",
      "epoch: 206\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0316\n",
      "epoch: 207\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0411\n",
      "epoch: 208\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0443\n",
      "epoch: 209\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0414\n",
      "epoch: 210\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0380\n",
      "epoch: 211\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0385\n",
      "epoch: 212\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0372\n",
      "epoch: 213\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 214\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0396\n",
      "epoch: 215\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0336\n",
      "epoch: 216\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0389\n",
      "epoch: 217\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0390\n",
      "epoch: 218\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0366\n",
      "epoch: 219\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0378\n",
      "epoch: 220\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0392\n",
      "epoch: 221\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0387\n",
      "epoch: 222\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0417\n",
      "epoch: 223\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0344\n",
      "epoch: 224\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0313\n",
      "epoch: 225\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0405\n",
      "epoch: 226\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0362\n",
      "epoch: 227\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0384\n",
      "epoch: 228\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0359\n",
      "epoch: 229\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0393\n",
      "epoch: 230\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0391\n",
      "epoch: 231\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0367\n",
      "epoch: 232\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0390\n",
      "epoch: 233\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0355\n",
      "epoch: 234\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 235\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0378\n",
      "epoch: 236\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0424\n",
      "epoch: 237\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0327\n",
      "epoch: 238\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0340\n",
      "epoch: 239\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0343\n",
      "epoch: 240\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0340\n",
      "epoch: 241\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0357\n",
      "epoch: 242\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0407\n",
      "epoch: 243\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0388\n",
      "epoch: 244\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0391\n",
      "epoch: 245\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0354\n",
      "epoch: 246\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0368\n",
      "epoch: 247\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0359\n",
      "epoch: 248\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0348\n",
      "epoch: 249\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0401\n",
      "epoch: 250\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0322\n",
      "epoch: 251\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 252\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0356\n",
      "epoch: 253\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0399\n",
      "epoch: 254\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0383\n",
      "epoch: 255\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0370\n",
      "epoch: 256\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0387\n",
      "epoch: 257\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0362\n",
      "epoch: 258\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0427\n",
      "epoch: 259\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0411\n",
      "epoch: 260\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0379\n",
      "epoch: 261\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0340\n",
      "epoch: 262\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0325\n",
      "epoch: 263\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0391\n",
      "epoch: 264\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0326\n",
      "epoch: 265\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0417\n",
      "epoch: 266\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0420\n",
      "epoch: 267\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0333\n",
      "epoch: 268\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0403\n",
      "epoch: 269\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0412\n",
      "epoch: 270\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0331\n",
      "epoch: 271\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0376\n",
      "epoch: 272\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0363\n",
      "epoch: 273\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0359\n",
      "epoch: 274\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0361\n",
      "epoch: 275\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0379\n",
      "epoch: 276\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0406\n",
      "epoch: 277\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0414\n",
      "epoch: 278\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0342\n",
      "epoch: 279\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0361\n",
      "epoch: 280\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0359\n",
      "epoch: 281\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0338\n",
      "epoch: 282\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0364\n",
      "epoch: 283\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0331\n",
      "epoch: 284\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0405\n",
      "epoch: 285\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0399\n",
      "epoch: 286\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0358\n",
      "epoch: 287\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0410\n",
      "epoch: 288\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0345\n",
      "epoch: 289\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0436\n",
      "epoch: 290\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0429\n",
      "epoch: 291\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0338\n",
      "epoch: 292\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0318\n",
      "epoch: 293\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0409\n",
      "epoch: 294\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0429\n",
      "epoch: 295\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0368\n",
      "epoch: 296\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0337\n",
      "epoch: 297\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0367\n",
      "epoch: 298\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0345\n",
      "epoch: 299\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0416\n",
      "epoch: 300\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0372\n",
      "epoch: 301\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0400\n",
      "epoch: 302\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0373\n",
      "epoch: 303\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0352\n",
      "epoch: 304\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0401\n",
      "epoch: 305\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0397\n",
      "epoch: 306\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0347\n",
      "epoch: 307\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0343\n",
      "epoch: 308\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0436\n",
      "epoch: 309\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 310\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0368\n",
      "epoch: 311\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0353\n",
      "epoch: 312\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0359\n",
      "epoch: 313\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0333\n",
      "epoch: 314\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0357\n",
      "epoch: 315\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0437\n",
      "epoch: 316\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0415\n",
      "epoch: 317\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0395\n",
      "epoch: 318\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0353\n",
      "epoch: 319\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0372\n",
      "epoch: 320\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0426\n",
      "epoch: 321\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0396\n",
      "epoch: 322\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0428\n",
      "epoch: 323\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 324\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0420\n",
      "epoch: 325\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0365\n",
      "epoch: 326\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0432\n",
      "epoch: 327\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0365\n",
      "epoch: 328\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0355\n",
      "epoch: 329\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0354\n",
      "epoch: 330\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0412\n",
      "epoch: 331\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0356\n",
      "epoch: 332\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0394\n",
      "epoch: 333\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0391\n",
      "epoch: 334\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0362\n",
      "epoch: 335\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0379\n",
      "epoch: 336\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0356\n",
      "epoch: 337\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0341\n",
      "epoch: 338\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0425\n",
      "epoch: 339\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0396\n",
      "epoch: 340\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 341\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0357\n",
      "epoch: 342\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0419\n",
      "epoch: 343\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 344\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0320\n",
      "epoch: 345\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0419\n",
      "epoch: 346\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0411\n",
      "epoch: 347\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0402\n",
      "epoch: 348\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0383\n",
      "epoch: 349\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0398\n",
      "epoch: 350\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0425\n",
      "epoch: 351\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0387\n",
      "epoch: 352\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0387\n",
      "epoch: 353\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0349\n",
      "epoch: 354\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0412\n",
      "epoch: 355\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0414\n",
      "epoch: 356\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0395\n",
      "epoch: 357\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0354\n",
      "epoch: 358\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0370\n",
      "epoch: 359\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0358\n",
      "epoch: 360\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0405\n",
      "epoch: 361\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0346\n",
      "epoch: 362\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0396\n",
      "epoch: 363\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0358\n",
      "epoch: 364\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0366\n",
      "epoch: 365\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0360\n",
      "epoch: 366\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0389\n",
      "epoch: 367\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0396\n",
      "epoch: 368\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0378\n",
      "epoch: 369\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0424\n",
      "epoch: 370\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0364\n",
      "epoch: 371\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0352\n",
      "epoch: 372\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0395\n",
      "epoch: 373\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0391\n",
      "epoch: 374\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0348\n",
      "epoch: 375\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0399\n",
      "epoch: 376\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0404\n",
      "epoch: 377\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0408\n",
      "epoch: 378\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0354\n",
      "epoch: 379\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0364\n",
      "epoch: 380\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0341\n",
      "epoch: 381\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0408\n",
      "epoch: 382\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0380\n",
      "epoch: 383\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0362\n",
      "epoch: 384\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0376\n",
      "epoch: 385\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0426\n",
      "epoch: 386\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0390\n",
      "epoch: 387\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0326\n",
      "epoch: 388\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0404\n",
      "epoch: 389\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0405\n",
      "epoch: 390\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0391\n",
      "epoch: 391\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0424\n",
      "epoch: 392\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0384\n",
      "epoch: 393\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0402\n",
      "epoch: 394\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0397\n",
      "epoch: 395\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0424\n",
      "epoch: 396\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0377\n",
      "epoch: 397\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0403\n",
      "epoch: 398\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0365\n",
      "epoch: 399\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0360\n",
      "epoch: 400\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0301\n",
      "Forecasting Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-04-01 12:29:56.048165: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=10.186012, Expected=9.510000\n",
      "Month=2, Predicted=9.822094, Expected=9.796000\n",
      "Month=3, Predicted=9.540120, Expected=9.468500\n",
      "Month=4, Predicted=9.515303, Expected=9.672000\n",
      "Month=5, Predicted=9.505069, Expected=9.610000\n",
      "Month=6, Predicted=9.539609, Expected=9.240000\n",
      "Month=7, Predicted=9.335134, Expected=10.318300\n",
      "Month=8, Predicted=9.869317, Expected=8.974800\n",
      "Month=9, Predicted=9.638060, Expected=9.114000\n",
      "Month=10, Predicted=8.943080, Expected=9.300000\n",
      "Month=11, Predicted=9.064524, Expected=8.400000\n",
      "Month=12, Predicted=8.815437, Expected=9.300000\n",
      "Month=13, Predicted=8.873924, Expected=9.000000\n",
      "Month=14, Predicted=9.051023, Expected=9.300000\n",
      "Month=15, Predicted=9.129847, Expected=9.460000\n",
      "Month=16, Predicted=9.292498, Expected=9.145000\n",
      "Month=17, Predicted=9.232645, Expected=9.021000\n",
      "Month=18, Predicted=8.967567, Expected=8.750000\n",
      "Month=19, Predicted=8.760191, Expected=8.710000\n",
      "Month=20, Predicted=8.618579, Expected=8.370000\n",
      "Month=21, Predicted=8.425838, Expected=8.504000\n",
      "Month=22, Predicted=8.338141, Expected=9.819700\n",
      "Month=23, Predicted=9.340830, Expected=9.827300\n",
      "Month=24, Predicted=9.794379, Expected=9.929800\n",
      "Month=25, Predicted=9.848822, Expected=9.288000\n",
      "Month=26, Predicted=9.506456, Expected=9.300000\n",
      "Month=27, Predicted=9.165865, Expected=9.060000\n",
      "Month=28, Predicted=9.050606, Expected=8.835000\n",
      "Month=29, Predicted=8.841552, Expected=8.388600\n",
      "Month=30, Predicted=8.483670, Expected=8.400000\n",
      "Month=31, Predicted=8.265482, Expected=8.525000\n",
      "Month=32, Predicted=8.368030, Expected=8.250000\n",
      "Month=33, Predicted=8.303375, Expected=8.419000\n",
      "Month=34, Predicted=8.247501, Expected=9.455000\n",
      "Month=35, Predicted=9.032656, Expected=8.540000\n",
      "Month=36, Predicted=8.976520, Expected=9.455000\n",
      "Month=37, Predicted=9.068733, Expected=9.000000\n",
      "Month=38, Predicted=9.116904, Expected=9.599000\n",
      "Month=39, Predicted=9.325259, Expected=9.436000\n",
      "Month=40, Predicted=9.415007, Expected=9.539800\n",
      "Month=41, Predicted=9.433851, Expected=9.028600\n",
      "Month=42, Predicted=9.176051, Expected=8.932000\n",
      "Month=43, Predicted=8.856091, Expected=8.993000\n",
      "Month=44, Predicted=8.846021, Expected=8.678400\n",
      "Month=45, Predicted=8.744019, Expected=9.011100\n",
      "Month=46, Predicted=8.775466, Expected=9.630000\n",
      "Month=47, Predicted=9.320476, Expected=8.590400\n",
      "Month=48, Predicted=9.094804, Expected=9.736300\n",
      "Month=49, Predicted=9.280063, Expected=9.384500\n",
      "Month=50, Predicted=9.453331, Expected=9.947200\n",
      "Month=51, Predicted=9.701461, Expected=9.577100\n",
      "Month=52, Predicted=9.656566, Expected=9.117200\n",
      "Month=53, Predicted=9.252931, Expected=9.122500\n",
      "Month=54, Predicted=8.972223, Expected=8.880000\n",
      "Month=55, Predicted=8.887729, Expected=8.709200\n",
      "Month=56, Predicted=8.686530, Expected=8.428200\n",
      "Month=57, Predicted=8.443846, Expected=9.907600\n",
      "Month=58, Predicted=9.397118, Expected=9.145000\n",
      "Month=59, Predicted=9.479140, Expected=8.498000\n",
      "Month=60, Predicted=8.738887, Expected=9.362000\n",
      "Month=61, Predicted=8.891232, Expected=9.000000\n",
      "Month=62, Predicted=9.113334, Expected=9.455000\n",
      "Month=63, Predicted=9.221886, Expected=9.300000\n",
      "Month=64, Predicted=9.274240, Expected=8.990000\n",
      "Month=65, Predicted=9.061453, Expected=8.990000\n",
      "Month=66, Predicted=8.865200, Expected=8.790000\n",
      "Month=67, Predicted=8.780617, Expected=8.835000\n",
      "Month=68, Predicted=8.718493, Expected=8.700000\n",
      "Month=69, Predicted=8.664413, Expected=8.935000\n",
      "Month=70, Predicted=8.748989, Expected=8.835000\n",
      "Month=71, Predicted=8.797833, Expected=8.265000\n",
      "Month=72, Predicted=8.459541, Expected=8.835000\n",
      "Month=73, Predicted=8.494012, Expected=8.550000\n",
      "Month=74, Predicted=8.602581, Expected=8.680000\n",
      "Month=75, Predicted=8.553869, Expected=8.400000\n",
      "Month=76, Predicted=8.425227, Expected=8.525000\n",
      "Month=77, Predicted=8.377224, Expected=8.370000\n",
      "Month=78, Predicted=8.343476, Expected=7.890000\n",
      "Month=79, Predicted=8.030355, Expected=7.812000\n",
      "Month=80, Predicted=7.712250, Expected=7.620000\n",
      "Month=81, Predicted=7.592404, Expected=7.718000\n",
      "Month=82, Predicted=7.579330, Expected=8.323500\n",
      "Month=83, Predicted=8.013764, Expected=6.860000\n",
      "Month=84, Predicted=7.601173, Expected=8.308000\n",
      "Month=85, Predicted=7.801354, Expected=8.100000\n",
      "Month=86, Predicted=8.081430, Expected=8.525000\n",
      "Month=87, Predicted=8.347424, Expected=8.250000\n",
      "Month=88, Predicted=8.277408, Expected=8.215000\n",
      "Month=89, Predicted=8.147613, Expected=8.122600\n",
      "Month=90, Predicted=8.053128, Expected=7.778100\n",
      "Month=91, Predicted=7.846751, Expected=7.954600\n",
      "Month=92, Predicted=7.765726, Expected=7.420000\n",
      "Month=93, Predicted=7.588521, Expected=7.538300\n",
      "Month=94, Predicted=7.376123, Expected=7.905000\n",
      "Month=95, Predicted=7.652856, Expected=7.140000\n",
      "Month=96, Predicted=7.473255, Expected=8.432000\n",
      "Month=97, Predicted=7.950105, Expected=7.710000\n",
      "Month=98, Predicted=7.993659, Expected=7.967000\n",
      "Month=99, Predicted=7.796593, Expected=7.320000\n",
      "Month=100, Predicted=7.518727, Expected=7.502000\n",
      "Month=101, Predicted=7.316281, Expected=7.409000\n",
      "Month=102, Predicted=7.338575, Expected=7.200600\n",
      "Month=103, Predicted=7.218797, Expected=7.865000\n",
      "Month=104, Predicted=7.520806, Expected=6.690000\n",
      "Month=105, Predicted=7.252485, Expected=6.879400\n",
      "Month=106, Predicted=6.674742, Expected=7.440000\n",
      "Month=107, Predicted=7.088980, Expected=6.860000\n",
      "Month=108, Predicted=7.116809, Expected=7.595000\n",
      "Month=109, Predicted=7.241966, Expected=7.200000\n",
      "Month=110, Predicted=7.301506, Expected=7.130000\n",
      "Month=111, Predicted=7.087237, Expected=6.900000\n",
      "Month=112, Predicted=6.881145, Expected=7.130000\n",
      "Month=113, Predicted=6.937400, Expected=7.130000\n",
      "Month=114, Predicted=7.046066, Expected=6.840000\n",
      "Month=115, Predicted=6.898161, Expected=7.006000\n",
      "Month=116, Predicted=6.825971, Expected=6.780000\n",
      "Month=117, Predicted=6.792421, Expected=7.089600\n",
      "Month=118, Predicted=6.877628, Expected=6.882000\n",
      "Month=119, Predicted=6.892446, Expected=6.446700\n",
      "Month=120, Predicted=6.567381, Expected=6.882000\n",
      "Month=121, Predicted=6.584564, Expected=6.600000\n",
      "Month=122, Predicted=6.652636, Expected=6.820000\n",
      "Month=123, Predicted=6.651266, Expected=6.600000\n",
      "Month=124, Predicted=6.604012, Expected=6.820000\n",
      "Month=125, Predicted=6.644308, Expected=6.665000\n",
      "Month=126, Predicted=6.645255, Expected=6.450000\n",
      "Month=127, Predicted=6.461535, Expected=6.665000\n",
      "Month=128, Predicted=6.464130, Expected=6.450000\n",
      "Month=129, Predicted=6.466171, Expected=6.722100\n",
      "Month=130, Predicted=6.525053, Expected=6.820000\n",
      "Month=131, Predicted=6.692947, Expected=6.160000\n",
      "Month=132, Predicted=6.419956, Expected=6.820000\n",
      "Month=133, Predicted=6.458773, Expected=6.480000\n",
      "Month=134, Predicted=6.556119, Expected=6.596900\n",
      "Month=135, Predicted=6.477775, Expected=6.492000\n",
      "Month=136, Predicted=6.429848, Expected=6.510000\n",
      "Month=137, Predicted=6.419204, Expected=6.339500\n",
      "Month=138, Predicted=6.321045, Expected=6.001600\n",
      "Month=139, Predicted=6.060439, Expected=6.107000\n",
      "Month=140, Predicted=5.941958, Expected=5.790000\n",
      "Month=141, Predicted=5.843104, Expected=5.885000\n",
      "Month=142, Predicted=5.744722, Expected=7.280000\n",
      "Month=143, Predicted=6.784654, Expected=5.941600\n",
      "Month=144, Predicted=6.622582, Expected=6.810000\n",
      "Month=145, Predicted=6.438125, Expected=6.182000\n",
      "Month=146, Predicted=6.369927, Expected=6.293000\n",
      "Month=147, Predicted=6.172731, Expected=6.118600\n",
      "Month=148, Predicted=6.072253, Expected=6.138000\n",
      "Month=149, Predicted=6.045686, Expected=6.107000\n",
      "Month=150, Predicted=6.022192, Expected=5.913000\n",
      "Month=151, Predicted=5.913356, Expected=6.141100\n",
      "Month=152, Predicted=5.946056, Expected=6.248000\n",
      "Month=153, Predicted=6.120925, Expected=5.829700\n",
      "Month=154, Predicted=5.958580, Expected=6.829300\n",
      "Month=155, Predicted=6.402358, Expected=6.694400\n",
      "Month=156, Predicted=6.690351, Expected=7.726200\n",
      "Month=157, Predicted=7.359667, Expected=7.054400\n",
      "Month=158, Predicted=7.317881, Expected=7.268900\n",
      "Month=159, Predicted=7.099696, Expected=7.020000\n",
      "Month=160, Predicted=7.014122, Expected=6.510000\n",
      "Month=161, Predicted=6.670139, Expected=6.370500\n",
      "Month=162, Predicted=6.289714, Expected=5.730000\n",
      "Month=163, Predicted=5.929001, Expected=5.828000\n",
      "Month=164, Predicted=5.650322, Expected=5.580000\n",
      "Month=165, Predicted=5.583447, Expected=5.709900\n",
      "Month=166, Predicted=5.565456, Expected=6.696000\n",
      "Month=167, Predicted=6.282924, Expected=6.248000\n",
      "Month=168, Predicted=6.426591, Expected=6.711600\n",
      "Month=169, Predicted=6.464012, Expected=6.600100\n",
      "Month=170, Predicted=6.552463, Expected=7.508200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=171, Predicted=7.149063, Expected=7.765000\n",
      "Month=172, Predicted=7.595715, Expected=7.285000\n",
      "Month=173, Predicted=7.480854, Expected=6.959500\n",
      "Month=174, Predicted=6.988146, Expected=6.450000\n",
      "Month=175, Predicted=6.557901, Expected=6.572000\n",
      "Month=176, Predicted=6.385111, Expected=6.600000\n",
      "Month=177, Predicted=6.489076, Expected=4.265300\n",
      "Month=178, Predicted=5.463796, Expected=7.367000\n",
      "Month=179, Predicted=6.742754, Expected=6.544000\n",
      "Month=180, Predicted=6.852767, Expected=6.940800\n",
      "Train RMSE: 0.4882\n",
      "Train RMSPE: 7.0120\n",
      "Train MAE: 0.33270\n",
      "Train MAPE: 4.36931\n",
      "Forecasting Testing Data\n",
      "Month=1, Predicted=6.761105, Expected=6.786000\n",
      "Month=2, Predicted=6.722011, Expected=6.981200\n",
      "Month=3, Predicted=6.838067, Expected=6.756000\n",
      "Month=4, Predicted=6.766688, Expected=6.733200\n",
      "Month=5, Predicted=6.650868, Expected=6.671200\n",
      "Month=6, Predicted=6.594610, Expected=6.295600\n",
      "Month=7, Predicted=6.381606, Expected=6.432500\n",
      "Month=8, Predicted=6.258021, Expected=6.153000\n",
      "Month=9, Predicted=6.186065, Expected=6.389500\n",
      "Month=10, Predicted=6.199895, Expected=7.192000\n",
      "Month=11, Predicted=6.826507, Expected=6.524000\n",
      "Month=12, Predicted=6.820004, Expected=7.238500\n",
      "Month=13, Predicted=6.898564, Expected=6.990000\n",
      "Month=14, Predicted=7.010275, Expected=7.254000\n",
      "Month=15, Predicted=7.089600, Expected=6.720000\n",
      "Month=16, Predicted=6.882772, Expected=6.944000\n",
      "Month=17, Predicted=6.745669, Expected=7.052500\n",
      "Month=18, Predicted=6.903591, Expected=6.690000\n",
      "Month=19, Predicted=6.797208, Expected=6.909900\n",
      "Month=20, Predicted=6.709090, Expected=6.819000\n",
      "Month=21, Predicted=6.766778, Expected=7.167200\n",
      "Month=22, Predicted=6.954985, Expected=7.254000\n",
      "Month=23, Predicted=7.140121, Expected=6.664000\n",
      "Month=24, Predicted=6.885973, Expected=7.393500\n",
      "Month=25, Predicted=7.017041, Expected=7.125000\n",
      "Month=26, Predicted=7.171348, Expected=7.347000\n",
      "Month=27, Predicted=7.194677, Expected=7.216500\n",
      "Month=28, Predicted=7.174354, Expected=7.254000\n",
      "Month=29, Predicted=7.157123, Expected=7.238500\n",
      "Month=30, Predicted=7.149093, Expected=6.990000\n",
      "Month=31, Predicted=7.017729, Expected=7.192000\n",
      "Month=32, Predicted=7.003466, Expected=6.900000\n",
      "Month=33, Predicted=6.948254, Expected=7.427300\n",
      "Month=34, Predicted=7.140523, Expected=7.300500\n",
      "Month=35, Predicted=7.280330, Expected=6.902000\n",
      "Month=36, Predicted=7.019904, Expected=7.409000\n",
      "Month=37, Predicted=7.093066, Expected=7.179000\n",
      "Month=38, Predicted=7.210364, Expected=7.424500\n",
      "Month=39, Predicted=7.252791, Expected=7.275000\n",
      "Month=40, Predicted=7.248413, Expected=7.316000\n",
      "Month=41, Predicted=7.216408, Expected=7.086300\n",
      "Month=42, Predicted=7.093511, Expected=7.020000\n",
      "Month=43, Predicted=6.948796, Expected=7.270500\n",
      "Month=44, Predicted=7.065918, Expected=7.168800\n",
      "Month=45, Predicted=7.141951, Expected=7.448600\n",
      "Month=46, Predicted=7.255159, Expected=7.440200\n",
      "Test RMSE: 0.2787\n",
      "Test RMSPE: 3.9282\n",
      "Test MAE: 0.21311\n",
      "Test MAPE: 3.02077\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate .py (dgru )\n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import GRU\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(GRU(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(GRU(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(GRU(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[3], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('epoch:',i+1)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\t\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\t\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\n",
    "\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.4f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.4f' % rmspe_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.4f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_test = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "\tprint('Test RMSPE: %.4f' % rmspe_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel('Time',fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\n",
    "\t \n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back= 2\n",
    "\tneurons=[ 3 , 1 , 4 ]\n",
    "\tn_epoch=400\n",
    "\texperiment(series,look_back,neurons,n_epoch)\n",
    "\n",
    "run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b2117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:From /Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 11:19:49.702524: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-18 11:19:49.702599: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_16450/405472425.py:26: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_16450/405472425.py:240: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_16450/405472425.py:144: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 11:19:50.338413: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-18 11:19:50.403874: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-18 11:19:50.696538: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 6s 23ms/sample - loss: 0.0482\n",
      "Epoch: 1\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0425\n",
      "Epoch: 2\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0385\n",
      "Epoch: 3\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0358\n",
      "Epoch: 4\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0342\n",
      "Epoch: 5\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0331\n",
      "Epoch: 6\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0324\n",
      "Epoch: 7\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0319\n",
      "Epoch: 8\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0315\n",
      "Epoch: 9\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0311\n",
      "Epoch: 10\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0307\n",
      "Epoch: 11\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0304\n",
      "Epoch: 12\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0300\n",
      "Epoch: 13\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0296\n",
      "Epoch: 14\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0292\n",
      "Epoch: 15\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0288\n",
      "Epoch: 16\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0285\n",
      "Epoch: 17\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0282\n",
      "Epoch: 18\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0280\n",
      "Epoch: 19\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0277\n",
      "Epoch: 20\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0275\n",
      "Epoch: 21\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0273\n",
      "Epoch: 22\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0271\n",
      "Epoch: 23\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0268\n",
      "Epoch: 24\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0266\n",
      "Epoch: 25\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0264\n",
      "Epoch: 26\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0262\n",
      "Epoch: 27\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0259\n",
      "Epoch: 28\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0257\n",
      "Epoch: 29\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0255\n",
      "Epoch: 30\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0253\n",
      "Epoch: 31\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0251\n",
      "Epoch: 32\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0248\n",
      "Epoch: 33\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0247\n",
      "Epoch: 34\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0244\n",
      "Epoch: 35\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0243\n",
      "Epoch: 36\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0241\n",
      "Epoch: 37\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0241\n",
      "Epoch: 38\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0239\n",
      "Epoch: 39\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0239\n",
      "Epoch: 40\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0237\n",
      "Epoch: 41\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0235\n",
      "Epoch: 42\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0232\n",
      "Epoch: 43\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0230\n",
      "Epoch: 44\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0228\n",
      "Epoch: 45\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0226\n",
      "Epoch: 46\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0225\n",
      "Epoch: 47\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0223\n",
      "Epoch: 48\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0223\n",
      "Epoch: 49\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0219\n",
      "Epoch: 50\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0218\n",
      "Epoch: 51\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0215\n",
      "Epoch: 52\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0216\n",
      "Epoch: 53\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0211\n",
      "Epoch: 54\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0212\n",
      "Epoch: 55\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0208\n",
      "Epoch: 56\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0209\n",
      "Epoch: 57\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0203\n",
      "Epoch: 58\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0206\n",
      "Epoch: 59\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0200\n",
      "Epoch: 60\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0201\n",
      "Epoch: 61\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0195\n",
      "Epoch: 62\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0197\n",
      "Epoch: 63\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0191\n",
      "Epoch: 64\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0191\n",
      "Epoch: 65\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0187\n",
      "Epoch: 66\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0187\n",
      "Epoch: 67\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0183\n",
      "Epoch: 68\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0184\n",
      "Epoch: 69\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0180\n",
      "Epoch: 70\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0179\n",
      "Epoch: 71\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0175\n",
      "Epoch: 72\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0176\n",
      "Epoch: 73\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0173\n",
      "Epoch: 74\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0173\n",
      "Epoch: 75\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0169\n",
      "Epoch: 76\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0167\n",
      "Epoch: 77\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0163\n",
      "Epoch: 78\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0164\n",
      "Epoch: 79\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0162\n",
      "Epoch: 80\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0159\n",
      "Epoch: 81\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0158\n",
      "Epoch: 82\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0156\n",
      "Epoch: 83\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0154\n",
      "Epoch: 84\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0153\n",
      "Epoch: 85\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0152\n",
      "Epoch: 86\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0150\n",
      "Epoch: 87\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0150\n",
      "Epoch: 88\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0148\n",
      "Epoch: 89\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0147\n",
      "Epoch: 90\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0144\n",
      "Epoch: 91\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0143\n",
      "Epoch: 92\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0142\n",
      "Epoch: 93\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0143\n",
      "Epoch: 94\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0139\n",
      "Epoch: 95\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0140\n",
      "Epoch: 96\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0138\n",
      "Epoch: 97\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0135\n",
      "Epoch: 98\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0133\n",
      "Epoch: 99\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0135\n",
      "Epoch: 100\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0130\n",
      "Epoch: 101\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0130\n",
      "Epoch: 102\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0129\n",
      "Epoch: 103\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0128\n",
      "Epoch: 104\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0127\n",
      "Epoch: 105\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0125\n",
      "Epoch: 106\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0127\n",
      "Epoch: 107\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0125\n",
      "Epoch: 108\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0122\n",
      "Epoch: 109\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0121\n",
      "Epoch: 110\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0116\n",
      "Epoch: 111\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0116\n",
      "Epoch: 112\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0116\n",
      "Epoch: 113\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0113\n",
      "Epoch: 114\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0113\n",
      "Epoch: 115\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0117\n",
      "Epoch: 116\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0109\n",
      "Epoch: 117\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0112\n",
      "Epoch: 118\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 35ms/sample - loss: 0.0105\n",
      "Epoch: 119\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0109\n",
      "Epoch: 120\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 37ms/sample - loss: 0.0104\n",
      "Epoch: 121\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0108\n",
      "Epoch: 122\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0104\n",
      "Epoch: 123\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0108\n",
      "Epoch: 124\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0099\n",
      "Epoch: 125\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0106\n",
      "Epoch: 126\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0097\n",
      "Epoch: 127\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0102\n",
      "Epoch: 128\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0097\n",
      "Epoch: 129\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0104\n",
      "Epoch: 130\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0095\n",
      "Epoch: 131\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0102\n",
      "Epoch: 132\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0094\n",
      "Epoch: 133\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0097\n",
      "Epoch: 134\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0093\n",
      "Epoch: 135\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0097\n",
      "Epoch: 136\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0089\n",
      "Epoch: 137\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0092\n",
      "Epoch: 138\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0091\n",
      "Epoch: 139\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0092\n",
      "Epoch: 140\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0086\n",
      "Epoch: 141\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0089\n",
      "Epoch: 142\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0087\n",
      "Epoch: 143\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0082\n",
      "Epoch: 144\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0086\n",
      "Epoch: 145\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0081\n",
      "Epoch: 146\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0082\n",
      "Epoch: 147\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0082\n",
      "Epoch: 148\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0094\n",
      "Epoch: 149\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0083\n",
      "Epoch: 150\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0086\n",
      "Epoch: 151\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0080\n",
      "Epoch: 152\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0076\n",
      "Epoch: 153\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0074\n",
      "Epoch: 154\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0078\n",
      "Epoch: 155\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0073\n",
      "Epoch: 156\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0073\n",
      "Epoch: 157\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0074\n",
      "Epoch: 158\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0072\n",
      "Epoch: 159\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0074\n",
      "Epoch: 160\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0069\n",
      "Epoch: 161\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0073\n",
      "Epoch: 162\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0068\n",
      "Epoch: 163\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0072\n",
      "Epoch: 164\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0067\n",
      "Epoch: 165\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0073\n",
      "Epoch: 166\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0068\n",
      "Epoch: 167\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0070\n",
      "Epoch: 168\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0065\n",
      "Epoch: 169\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0063\n",
      "Epoch: 170\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0064\n",
      "Epoch: 171\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0065\n",
      "Epoch: 172\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0062\n",
      "Epoch: 173\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0068\n",
      "Epoch: 174\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0057\n",
      "Epoch: 175\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0068\n",
      "Epoch: 176\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0059\n",
      "Epoch: 177\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0057\n",
      "Epoch: 178\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0057\n",
      "Epoch: 179\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 35ms/sample - loss: 0.0059\n",
      "Epoch: 180\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0061\n",
      "Epoch: 181\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 36ms/sample - loss: 0.0057\n",
      "Epoch: 182\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0066\n",
      "Epoch: 183\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0055\n",
      "Epoch: 184\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0056\n",
      "Epoch: 185\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0056\n",
      "Epoch: 186\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0056\n",
      "Epoch: 187\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0053\n",
      "Epoch: 188\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0049\n",
      "Epoch: 189\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0050\n",
      "Epoch: 190\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0054\n",
      "Epoch: 191\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0054\n",
      "Epoch: 192\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0061\n",
      "Epoch: 193\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0084\n",
      "Epoch: 194\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0047\n",
      "Epoch: 195\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0048\n",
      "Epoch: 196\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0047\n",
      "Epoch: 197\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0052\n",
      "Epoch: 198\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0054\n",
      "Epoch: 199\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0043\n",
      "Epoch: 200\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0048\n",
      "Epoch: 201\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0052\n",
      "Epoch: 202\n",
      "Train on 180 samples\n",
      "141/180 [======================>.......] - ETA: 1s - loss: 0.0051"
     ]
    }
   ],
   "source": [
    "# oil static.py d-lstm \n",
    "\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "#from deap import base, creator, tools, algorithms\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import random\n",
    "\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, activation='relu', )))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')   \n",
    "    for i in range(nb_epoch):\n",
    "        print('Epoch:',i)\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "        model.reset_states()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\t\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\t\n",
    "\t\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t\n",
    "\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.5f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\tRMSPE_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.5f' % RMSPE_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.5f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\tRMSPE_test = RMSPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test RMSPE: %.5f' % RMSPE_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel('Time',fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back=5\n",
    "\tneurons=[5,4,2] \n",
    "\tn_epochs=800\n",
    "\t\n",
    "\t\n",
    "\n",
    "\texperiment(series,look_back,neurons,n_epochs)\n",
    "\n",
    "\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cb9e241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 12:33:17.523487: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 12:33:17.523518: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/56519950.py:28: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/56519950.py:250: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 12:33:17.887686: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 12:33:17.922504: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 12:33:17.967371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 6s 28ms/sample - loss: 0.0502\n",
      "Epoch: 1\n",
      "Train on 180 samples\n",
      "  1/180 [..............................] - ETA: 6s - loss: 0.1259"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 12:33:23.617962: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 12:33:23.651534: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 12:33:23.685405: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 12:33:23.719591: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0502\n",
      "Epoch: 2\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0493\n",
      "Epoch: 3\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0491\n",
      "Epoch: 4\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0491\n",
      "Epoch: 5\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0495\n",
      "Epoch: 6\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0482\n",
      "Epoch: 7\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0484\n",
      "Epoch: 8\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0480\n",
      "Epoch: 9\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0477\n",
      "Epoch: 10\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0488\n",
      "Epoch: 11\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0487\n",
      "Epoch: 12\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0487\n",
      "Epoch: 13\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0484\n",
      "Epoch: 14\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0471\n",
      "Epoch: 15\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0468\n",
      "Epoch: 16\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0467\n",
      "Epoch: 17\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0464\n",
      "Epoch: 18\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0479\n",
      "Epoch: 19\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0460\n",
      "Epoch: 20\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0464\n",
      "Epoch: 21\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0466\n",
      "Epoch: 22\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0432\n",
      "Epoch: 23\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0449\n",
      "Epoch: 24\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0432\n",
      "Epoch: 25\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0448\n",
      "Epoch: 26\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0442\n",
      "Epoch: 27\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0443\n",
      "Epoch: 28\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0407\n",
      "Epoch: 29\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0392\n",
      "Epoch: 30\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0388\n",
      "Epoch: 31\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0394\n",
      "Epoch: 32\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0397\n",
      "Epoch: 33\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0437\n",
      "Epoch: 34\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0408\n",
      "Epoch: 35\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0395\n",
      "Epoch: 36\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0413\n",
      "Epoch: 37\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0375\n",
      "Epoch: 38\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0404\n",
      "Epoch: 39\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0408\n",
      "Epoch: 40\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0376\n",
      "Epoch: 41\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0423\n",
      "Epoch: 42\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0422\n",
      "Epoch: 43\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0340\n",
      "Epoch: 44\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0430\n",
      "Epoch: 45\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0376\n",
      "Epoch: 46\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0389\n",
      "Epoch: 47\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0346\n",
      "Epoch: 48\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0389\n",
      "Epoch: 49\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0404\n",
      "Epoch: 50\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0374\n",
      "Epoch: 51\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0361\n",
      "Epoch: 52\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0348\n",
      "Epoch: 53\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0382\n",
      "Epoch: 54\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0365\n",
      "Epoch: 55\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0406\n",
      "Epoch: 56\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0352\n",
      "Epoch: 57\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0386\n",
      "Epoch: 58\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0394\n",
      "Epoch: 59\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0335\n",
      "Epoch: 60\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0344\n",
      "Epoch: 61\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0392\n",
      "Epoch: 62\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0398\n",
      "Epoch: 63\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0382\n",
      "Epoch: 64\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0377\n",
      "Epoch: 65\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0336\n",
      "Epoch: 66\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0367\n",
      "Epoch: 67\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0375\n",
      "Epoch: 68\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0353\n",
      "Epoch: 69\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0402\n",
      "Epoch: 70\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0329\n",
      "Epoch: 71\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0363\n",
      "Epoch: 72\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0388\n",
      "Epoch: 73\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0388\n",
      "Epoch: 74\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0363\n",
      "Epoch: 75\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0402\n",
      "Epoch: 76\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0391\n",
      "Epoch: 77\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0340\n",
      "Epoch: 78\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0392\n",
      "Epoch: 79\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0388\n",
      "Epoch: 80\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0384\n",
      "Epoch: 81\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0347\n",
      "Epoch: 82\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0327\n",
      "Epoch: 83\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0390\n",
      "Epoch: 84\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0365\n",
      "Epoch: 85\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0353\n",
      "Epoch: 86\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0356\n",
      "Epoch: 87\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0332\n",
      "Epoch: 88\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0311\n",
      "Epoch: 89\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0344\n",
      "Epoch: 90\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 406s 2s/sample - loss: 0.0359\n",
      "Epoch: 91\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0330\n",
      "Epoch: 92\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0357\n",
      "Epoch: 93\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0369\n",
      "Epoch: 94\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0321\n",
      "Epoch: 95\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0364\n",
      "Epoch: 96\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0415\n",
      "Epoch: 97\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0351\n",
      "Epoch: 98\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0336\n",
      "Epoch: 99\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0375\n",
      "Epoch: 100\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0380\n",
      "Epoch: 101\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0305\n",
      "Epoch: 102\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0334\n",
      "Epoch: 103\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0374\n",
      "Epoch: 104\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0362\n",
      "Epoch: 105\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0410\n",
      "Epoch: 106\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0378\n",
      "Epoch: 107\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0374\n",
      "Epoch: 108\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0378\n",
      "Epoch: 109\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0328\n",
      "Epoch: 110\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0312\n",
      "Epoch: 111\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0398\n",
      "Epoch: 112\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0353\n",
      "Epoch: 113\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0328\n",
      "Epoch: 114\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0318\n",
      "Epoch: 115\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0341\n",
      "Epoch: 116\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0353\n",
      "Epoch: 117\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0368\n",
      "Epoch: 118\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0345\n",
      "Epoch: 119\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0334\n",
      "Epoch: 120\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0348\n",
      "Epoch: 121\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0350\n",
      "Epoch: 122\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0316\n",
      "Epoch: 123\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0357\n",
      "Epoch: 124\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0331\n",
      "Epoch: 125\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0375\n",
      "Epoch: 126\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0321\n",
      "Epoch: 127\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0322\n",
      "Epoch: 128\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0328\n",
      "Epoch: 129\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0350\n",
      "Epoch: 130\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0348\n",
      "Epoch: 131\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0369\n",
      "Epoch: 132\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0341\n",
      "Epoch: 133\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0349\n",
      "Epoch: 134\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0337\n",
      "Epoch: 135\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0305\n",
      "Epoch: 136\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0313\n",
      "Epoch: 137\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0331\n",
      "Epoch: 138\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0363\n",
      "Epoch: 139\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0358\n",
      "Epoch: 140\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0340\n",
      "Epoch: 141\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0337\n",
      "Epoch: 142\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0329\n",
      "Epoch: 143\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0318\n",
      "Epoch: 144\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0334\n",
      "Epoch: 145\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0377\n",
      "Epoch: 146\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0356\n",
      "Epoch: 147\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 240s 1s/sample - loss: 0.0358\n",
      "Epoch: 148\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0334\n",
      "Epoch: 149\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0325\n",
      "Epoch: 150\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0353\n",
      "Epoch: 151\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0321\n",
      "Epoch: 152\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0362\n",
      "Epoch: 153\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0302\n",
      "Epoch: 154\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0316\n",
      "Epoch: 155\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0343\n",
      "Epoch: 156\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0366\n",
      "Epoch: 157\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0371\n",
      "Epoch: 158\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0346\n",
      "Epoch: 159\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0338\n",
      "Epoch: 160\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0324\n",
      "Epoch: 161\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0323\n",
      "Epoch: 162\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0345\n",
      "Epoch: 163\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0309\n",
      "Epoch: 164\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0384\n",
      "Epoch: 165\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0353\n",
      "Epoch: 166\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0316\n",
      "Epoch: 167\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0313\n",
      "Epoch: 168\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0335\n",
      "Epoch: 169\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0327\n",
      "Epoch: 170\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0334\n",
      "Epoch: 171\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0349\n",
      "Epoch: 172\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0307\n",
      "Epoch: 173\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0409\n",
      "Epoch: 174\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0357\n",
      "Epoch: 175\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0326\n",
      "Epoch: 176\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0353\n",
      "Epoch: 177\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0353\n",
      "Epoch: 178\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0355\n",
      "Epoch: 179\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 25ms/sample - loss: 0.0335\n",
      "Epoch: 180\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 21ms/sample - loss: 0.0349\n",
      "Epoch: 181\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0329\n",
      "Epoch: 182\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 27ms/sample - loss: 0.0346\n",
      "Epoch: 183\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 25ms/sample - loss: 0.0329\n",
      "Epoch: 184\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0336\n",
      "Epoch: 185\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0314\n",
      "Epoch: 186\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0344\n",
      "Epoch: 187\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 26ms/sample - loss: 0.0332\n",
      "Epoch: 188\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0313\n",
      "Epoch: 189\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0340\n",
      "Epoch: 190\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0328\n",
      "Epoch: 191\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0341\n",
      "Epoch: 192\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0340\n",
      "Epoch: 193\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0375\n",
      "Epoch: 194\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 23ms/sample - loss: 0.0341\n",
      "Epoch: 195\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0328\n",
      "Epoch: 196\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0346\n",
      "Epoch: 197\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 24ms/sample - loss: 0.0382\n",
      "Epoch: 198\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 4s 22ms/sample - loss: 0.0323\n",
      "Epoch: 199\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 30s 166ms/sample - loss: 0.0360\n",
      "Forecasting Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-04-01 12:58:02.869170: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=10.226778, Expected=9.510000\n",
      "Month=2, Predicted=9.922586, Expected=9.796000\n",
      "Month=3, Predicted=9.603691, Expected=9.468500\n",
      "Month=4, Predicted=9.587510, Expected=9.672000\n",
      "Month=5, Predicted=9.625954, Expected=9.610000\n",
      "Month=6, Predicted=9.537663, Expected=9.240000\n",
      "Month=7, Predicted=9.425887, Expected=10.318300\n",
      "Month=8, Predicted=9.880555, Expected=8.974800\n",
      "Month=9, Predicted=9.678230, Expected=9.114000\n",
      "Month=10, Predicted=8.998434, Expected=9.300000\n",
      "Month=11, Predicted=9.118868, Expected=8.400000\n",
      "Month=12, Predicted=9.183954, Expected=9.300000\n",
      "Month=13, Predicted=8.934116, Expected=9.000000\n",
      "Month=14, Predicted=9.031167, Expected=9.300000\n",
      "Month=15, Predicted=9.229735, Expected=9.460000\n",
      "Month=16, Predicted=9.242818, Expected=9.145000\n",
      "Month=17, Predicted=9.269927, Expected=9.021000\n",
      "Month=18, Predicted=8.997294, Expected=8.750000\n",
      "Month=19, Predicted=8.815190, Expected=8.710000\n",
      "Month=20, Predicted=8.701921, Expected=8.370000\n",
      "Month=21, Predicted=8.510809, Expected=8.504000\n",
      "Month=22, Predicted=8.405638, Expected=9.819700\n",
      "Month=23, Predicted=9.312735, Expected=9.827300\n",
      "Month=24, Predicted=9.636096, Expected=9.929800\n",
      "Month=25, Predicted=9.765465, Expected=9.288000\n",
      "Month=26, Predicted=9.499912, Expected=9.300000\n",
      "Month=27, Predicted=9.223860, Expected=9.060000\n",
      "Month=28, Predicted=9.119511, Expected=8.835000\n",
      "Month=29, Predicted=8.994639, Expected=8.388600\n",
      "Month=30, Predicted=8.586208, Expected=8.400000\n",
      "Month=31, Predicted=8.353178, Expected=8.525000\n",
      "Month=32, Predicted=8.416184, Expected=8.250000\n",
      "Month=33, Predicted=8.399610, Expected=8.419000\n",
      "Month=34, Predicted=8.276241, Expected=9.455000\n",
      "Month=35, Predicted=8.985767, Expected=8.540000\n",
      "Month=36, Predicted=8.934040, Expected=9.455000\n",
      "Month=37, Predicted=9.074986, Expected=9.000000\n",
      "Month=38, Predicted=9.069888, Expected=9.599000\n",
      "Month=39, Predicted=9.396163, Expected=9.436000\n",
      "Month=40, Predicted=9.348282, Expected=9.539800\n",
      "Month=41, Predicted=9.479740, Expected=9.028600\n",
      "Month=42, Predicted=9.195450, Expected=8.932000\n",
      "Month=43, Predicted=8.931909, Expected=8.993000\n",
      "Month=44, Predicted=8.886865, Expected=8.678400\n",
      "Month=45, Predicted=8.865866, Expected=9.011100\n",
      "Month=46, Predicted=8.810455, Expected=9.630000\n",
      "Month=47, Predicted=9.265081, Expected=8.590400\n",
      "Month=48, Predicted=9.157092, Expected=9.736300\n",
      "Month=49, Predicted=9.317412, Expected=9.384500\n",
      "Month=50, Predicted=9.395096, Expected=9.947200\n",
      "Month=51, Predicted=9.774084, Expected=9.577100\n",
      "Month=52, Predicted=9.599507, Expected=9.117200\n",
      "Month=53, Predicted=9.365275, Expected=9.122500\n",
      "Month=54, Predicted=9.029300, Expected=8.880000\n",
      "Month=55, Predicted=8.993512, Expected=8.709200\n",
      "Month=56, Predicted=8.805934, Expected=8.428200\n",
      "Month=57, Predicted=8.520889, Expected=9.907600\n",
      "Month=58, Predicted=9.423183, Expected=9.145000\n",
      "Month=59, Predicted=9.379245, Expected=8.498000\n",
      "Month=60, Predicted=8.862209, Expected=9.362000\n",
      "Month=61, Predicted=8.971293, Expected=9.000000\n",
      "Month=62, Predicted=9.221292, Expected=9.455000\n",
      "Month=63, Predicted=9.291778, Expected=9.300000\n",
      "Month=64, Predicted=9.220124, Expected=8.990000\n",
      "Month=65, Predicted=9.142529, Expected=8.990000\n",
      "Month=66, Predicted=8.897761, Expected=8.790000\n",
      "Month=67, Predicted=8.842937, Expected=8.835000\n",
      "Month=68, Predicted=8.785521, Expected=8.700000\n",
      "Month=69, Predicted=8.697793, Expected=8.935000\n",
      "Month=70, Predicted=8.776830, Expected=8.835000\n",
      "Month=71, Predicted=8.790995, Expected=8.265000\n",
      "Month=72, Predicted=8.548144, Expected=8.835000\n",
      "Month=73, Predicted=8.530410, Expected=8.550000\n",
      "Month=74, Predicted=8.615395, Expected=8.680000\n",
      "Month=75, Predicted=8.639071, Expected=8.400000\n",
      "Month=76, Predicted=8.432909, Expected=8.525000\n",
      "Month=77, Predicted=8.429480, Expected=8.370000\n",
      "Month=78, Predicted=8.360853, Expected=7.890000\n",
      "Month=79, Predicted=8.142070, Expected=7.812000\n",
      "Month=80, Predicted=7.779491, Expected=7.620000\n",
      "Month=81, Predicted=7.675339, Expected=7.718000\n",
      "Month=82, Predicted=7.668006, Expected=8.323500\n",
      "Month=83, Predicted=7.998470, Expected=6.860000\n",
      "Month=84, Predicted=7.722816, Expected=8.308000\n",
      "Month=85, Predicted=7.879845, Expected=8.100000\n",
      "Month=86, Predicted=8.038450, Expected=8.525000\n",
      "Month=87, Predicted=8.433523, Expected=8.250000\n",
      "Month=88, Predicted=8.231094, Expected=8.215000\n",
      "Month=89, Predicted=8.186161, Expected=8.122600\n",
      "Month=90, Predicted=8.066436, Expected=7.778100\n",
      "Month=91, Predicted=7.938133, Expected=7.954600\n",
      "Month=92, Predicted=7.811229, Expected=7.420000\n",
      "Month=93, Predicted=7.672199, Expected=7.538300\n",
      "Month=94, Predicted=7.457862, Expected=7.905000\n",
      "Month=95, Predicted=7.645625, Expected=7.140000\n",
      "Month=96, Predicted=7.620399, Expected=8.432000\n",
      "Month=97, Predicted=7.988338, Expected=7.710000\n",
      "Month=98, Predicted=7.926636, Expected=7.967000\n",
      "Month=99, Predicted=7.877298, Expected=7.320000\n",
      "Month=100, Predicted=7.585338, Expected=7.502000\n",
      "Month=101, Predicted=7.433218, Expected=7.409000\n",
      "Month=102, Predicted=7.368080, Expected=7.200600\n",
      "Month=103, Predicted=7.355382, Expected=7.865000\n",
      "Month=104, Predicted=7.522882, Expected=6.690000\n",
      "Month=105, Predicted=7.332218, Expected=6.879400\n",
      "Month=106, Predicted=6.737680, Expected=7.440000\n",
      "Month=107, Predicted=7.092705, Expected=6.860000\n",
      "Month=108, Predicted=7.348332, Expected=7.595000\n",
      "Month=109, Predicted=7.257978, Expected=7.200000\n",
      "Month=110, Predicted=7.244001, Expected=7.130000\n",
      "Month=111, Predicted=7.188034, Expected=6.900000\n",
      "Month=112, Predicted=6.916255, Expected=7.130000\n",
      "Month=113, Predicted=6.998014, Expected=7.130000\n",
      "Month=114, Predicted=7.052154, Expected=6.840000\n",
      "Month=115, Predicted=6.957619, Expected=7.006000\n",
      "Month=116, Predicted=6.845315, Expected=6.780000\n",
      "Month=117, Predicted=6.819049, Expected=7.089600\n",
      "Month=118, Predicted=6.914970, Expected=6.882000\n",
      "Month=119, Predicted=6.881222, Expected=6.446700\n",
      "Month=120, Predicted=6.659778, Expected=6.882000\n",
      "Month=121, Predicted=6.615100, Expected=6.600000\n",
      "Month=122, Predicted=6.686553, Expected=6.820000\n",
      "Month=123, Predicted=6.712984, Expected=6.600000\n",
      "Month=124, Predicted=6.596369, Expected=6.820000\n",
      "Month=125, Predicted=6.680114, Expected=6.665000\n",
      "Month=126, Predicted=6.637299, Expected=6.450000\n",
      "Month=127, Predicted=6.524818, Expected=6.665000\n",
      "Month=128, Predicted=6.480829, Expected=6.450000\n",
      "Month=129, Predicted=6.498878, Expected=6.722100\n",
      "Month=130, Predicted=6.554887, Expected=6.820000\n",
      "Month=131, Predicted=6.656586, Expected=6.160000\n",
      "Month=132, Predicted=6.503731, Expected=6.820000\n",
      "Month=133, Predicted=6.491852, Expected=6.480000\n",
      "Month=134, Predicted=6.549937, Expected=6.596900\n",
      "Month=135, Predicted=6.576163, Expected=6.492000\n",
      "Month=136, Predicted=6.427646, Expected=6.510000\n",
      "Month=137, Predicted=6.471921, Expected=6.339500\n",
      "Month=138, Predicted=6.343974, Expected=6.001600\n",
      "Month=139, Predicted=6.134597, Expected=6.107000\n",
      "Month=140, Predicted=5.989881, Expected=5.790000\n",
      "Month=141, Predicted=5.915370, Expected=5.885000\n",
      "Month=142, Predicted=5.817242, Expected=7.280000\n",
      "Month=143, Predicted=6.756334, Expected=5.941600\n",
      "Month=144, Predicted=6.577618, Expected=6.810000\n",
      "Month=145, Predicted=6.451937, Expected=6.182000\n",
      "Month=146, Predicted=6.428901, Expected=6.293000\n",
      "Month=147, Predicted=6.319834, Expected=6.118600\n",
      "Month=148, Predicted=6.102767, Expected=6.138000\n",
      "Month=149, Predicted=6.152615, Expected=6.107000\n",
      "Month=150, Predicted=6.041243, Expected=5.913000\n",
      "Month=151, Predicted=5.965539, Expected=6.141100\n",
      "Month=152, Predicted=5.965021, Expected=6.248000\n",
      "Month=153, Predicted=6.106502, Expected=5.829700\n",
      "Month=154, Predicted=6.011753, Expected=6.829300\n",
      "Month=155, Predicted=6.408499, Expected=6.694400\n",
      "Month=156, Predicted=6.578206, Expected=7.726200\n",
      "Month=157, Predicted=7.332859, Expected=7.054400\n",
      "Month=158, Predicted=7.143940, Expected=7.268900\n",
      "Month=159, Predicted=7.116908, Expected=7.020000\n",
      "Month=160, Predicted=7.028307, Expected=6.510000\n",
      "Month=161, Predicted=6.858957, Expected=6.370500\n",
      "Month=162, Predicted=6.366896, Expected=5.730000\n",
      "Month=163, Predicted=6.096397, Expected=5.828000\n",
      "Month=164, Predicted=5.766586, Expected=5.580000\n",
      "Month=165, Predicted=5.671959, Expected=5.709900\n",
      "Month=166, Predicted=5.675308, Expected=6.696000\n",
      "Month=167, Predicted=6.248884, Expected=6.248000\n",
      "Month=168, Predicted=6.341114, Expected=6.711600\n",
      "Month=169, Predicted=6.442479, Expected=6.600100\n",
      "Month=170, Predicted=6.488353, Expected=7.508200\n",
      "Month=171, Predicted=7.143810, Expected=7.765000\n",
      "Month=172, Predicted=7.429883, Expected=7.285000\n",
      "Month=173, Predicted=7.444319, Expected=6.959500\n",
      "Month=174, Predicted=7.025675, Expected=6.450000\n",
      "Month=175, Predicted=6.664237, Expected=6.572000\n",
      "Month=176, Predicted=6.493121, Expected=6.600000\n",
      "Month=177, Predicted=6.553833, Expected=4.265300\n",
      "Month=178, Predicted=5.920922, Expected=7.367000\n",
      "Month=179, Predicted=7.000131, Expected=6.544000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=180, Predicted=6.979507, Expected=6.940800\n",
      "Train RMSE: 0.274\n",
      "Train RMSPE: 4.318\n",
      "Train MAE: 0.19074\n",
      "Train MAPE: 2.55713\n",
      "Forecasting Testing Data\n",
      "Month=1, Predicted=6.734055, Expected=6.786000\n",
      "Month=2, Predicted=6.765644, Expected=6.981200\n",
      "Month=3, Predicted=6.949895, Expected=6.756000\n",
      "Month=4, Predicted=6.731980, Expected=6.733200\n",
      "Month=5, Predicted=6.706539, Expected=6.671200\n",
      "Month=6, Predicted=6.606950, Expected=6.295600\n",
      "Month=7, Predicted=6.499756, Expected=6.432500\n",
      "Month=8, Predicted=6.227628, Expected=6.153000\n",
      "Month=9, Predicted=6.219903, Expected=6.389500\n",
      "Month=10, Predicted=6.239065, Expected=7.192000\n",
      "Month=11, Predicted=6.754457, Expected=6.524000\n",
      "Month=12, Predicted=6.890707, Expected=7.238500\n",
      "Month=13, Predicted=6.826647, Expected=6.990000\n",
      "Month=14, Predicted=6.954852, Expected=7.254000\n",
      "Month=15, Predicted=7.225271, Expected=6.720000\n",
      "Month=16, Predicted=6.862989, Expected=6.944000\n",
      "Month=17, Predicted=6.813836, Expected=7.052500\n",
      "Month=18, Predicted=6.859995, Expected=6.690000\n",
      "Month=19, Predicted=6.956691, Expected=6.909900\n",
      "Month=20, Predicted=6.719149, Expected=6.819000\n",
      "Month=21, Predicted=6.758979, Expected=7.167200\n",
      "Month=22, Predicted=6.968539, Expected=7.254000\n",
      "Month=23, Predicted=7.111673, Expected=6.664000\n",
      "Month=24, Predicted=6.934853, Expected=7.393500\n",
      "Month=25, Predicted=7.010952, Expected=7.125000\n",
      "Month=26, Predicted=7.165856, Expected=7.347000\n",
      "Month=27, Predicted=7.317432, Expected=7.216500\n",
      "Month=28, Predicted=7.146472, Expected=7.254000\n",
      "Month=29, Predicted=7.244276, Expected=7.238500\n",
      "Month=30, Predicted=7.152025, Expected=6.990000\n",
      "Month=31, Predicted=7.096449, Expected=7.192000\n",
      "Month=32, Predicted=7.013617, Expected=6.900000\n",
      "Month=33, Predicted=7.028142, Expected=7.427300\n",
      "Month=34, Predicted=7.175173, Expected=7.300500\n",
      "Month=35, Predicted=7.248370, Expected=6.902000\n",
      "Month=36, Predicted=7.167571, Expected=7.409000\n",
      "Month=37, Predicted=7.062336, Expected=7.179000\n",
      "Month=38, Predicted=7.286883, Expected=7.424500\n",
      "Month=39, Predicted=7.358130, Expected=7.275000\n",
      "Month=40, Predicted=7.198115, Expected=7.316000\n",
      "Month=41, Predicted=7.284489, Expected=7.086300\n",
      "Month=42, Predicted=7.113652, Expected=7.020000\n",
      "Month=43, Predicted=7.022254, Expected=7.270500\n",
      "Month=44, Predicted=7.104899, Expected=7.168800\n",
      "Month=45, Predicted=7.206861, Expected=7.448600\n",
      "Month=46, Predicted=7.267602, Expected=7.440200\n",
      "Test RMSE: 0.262\n",
      "Test RMSPE: 3.714\n",
      "Test MAE: 0.20266\n",
      "Test MAPE: 2.87835\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# oil dynamic (d-lstm )\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.constraints import max_norm\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]) , stateful=True,return_sequences = True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(LSTM(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t#model.add(LSTM(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t#model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('Epoch:',i)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\t\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "\n",
    "# Update LSTM model\n",
    "def update_model(model, train, batch_size, updates):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tfor i in range(updates):\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment(series, updates,look_back,neurons,n_epoch):\n",
    "\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[:len(train_scaled)], predictions_train))\n",
    "\tprint('Train RMSE: %.3f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_train = RMSPE(raw_values[:len(train_scaled)],predictions_train)\n",
    "\tprint('Train RMSPE: %.3f' % rmspe_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[:len(train_scaled)], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[:len(train_scaled)], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\ttrain_copy = np.copy(train_scaled)\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# update model\n",
    "\t\tif i > 0:\n",
    "\t\t\tupdate_model(lstm_model, train_copy, 1, updates)\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\t# add to training set\n",
    "\t\ttrain_copy = concatenate((train_copy, test_scaled[i,:].reshape(1, -1)))\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.3f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_test = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "\tprint('Test RMSPE: %.3f' % rmspe_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel(\"Time\",fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\t \n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back= 4\n",
    "\tneurons= [ 2 , 5 ]\n",
    "\tn_epochs=200\n",
    "\tupdates= 1\n",
    "\t\n",
    "\n",
    "\texperiment(series, updates,look_back,neurons,n_epochs)\n",
    "\n",
    "\n",
    "run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e537010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:31:33.767375: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 17:31:33.768922: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/3729668406.py:27: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/3729668406.py:242: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv(\"oil_production.csv\", header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:31:34.312754: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:31:34.392459: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:31:34.709500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 7s 31ms/sample - loss: 1.7483\n",
      "epoch: 2\n",
      "Train on 180 samples\n",
      "  3/180 [..............................] - ETA: 5s - loss: 0.5913"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:31:40.804166: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:31:40.847196: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:31:40.891262: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 5s 29ms/sample - loss: 1.1794\n",
      "epoch: 3\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.5794\n",
      "epoch: 4\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.5131\n",
      "epoch: 5\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 37ms/sample - loss: 0.3202\n",
      "epoch: 6\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.3416\n",
      "epoch: 7\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.2483\n",
      "epoch: 8\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.1888\n",
      "epoch: 9\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.1353\n",
      "epoch: 10\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.1211\n",
      "epoch: 11\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.1111\n",
      "epoch: 12\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0804\n",
      "epoch: 13\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0905\n",
      "epoch: 14\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0780\n",
      "epoch: 15\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0881\n",
      "epoch: 16\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0713\n",
      "epoch: 17\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0653\n",
      "epoch: 18\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0580\n",
      "epoch: 19\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0616\n",
      "epoch: 20\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0567\n",
      "epoch: 21\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0528\n",
      "epoch: 22\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0494\n",
      "epoch: 23\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0532\n",
      "epoch: 24\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0559\n",
      "epoch: 25\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0437\n",
      "epoch: 26\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0492\n",
      "epoch: 27\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0440\n",
      "epoch: 28\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0520\n",
      "epoch: 29\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 36ms/sample - loss: 0.0443\n",
      "epoch: 30\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 36ms/sample - loss: 0.0485\n",
      "epoch: 31\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0422\n",
      "epoch: 32\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0492\n",
      "epoch: 33\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0419\n",
      "epoch: 34\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0456\n",
      "epoch: 35\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0466\n",
      "epoch: 36\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0416\n",
      "epoch: 37\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0455\n",
      "epoch: 38\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0461\n",
      "epoch: 39\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 36ms/sample - loss: 0.0418\n",
      "epoch: 40\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0446\n",
      "epoch: 41\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0403\n",
      "epoch: 42\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0422\n",
      "epoch: 43\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0423\n",
      "epoch: 44\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0432\n",
      "epoch: 45\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0410\n",
      "epoch: 46\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0423\n",
      "epoch: 47\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 35ms/sample - loss: 0.0394\n",
      "epoch: 48\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0380\n",
      "epoch: 49\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0411\n",
      "epoch: 50\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0438\n",
      "epoch: 51\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0382\n",
      "epoch: 52\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0450\n",
      "epoch: 53\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0408\n",
      "epoch: 54\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0393\n",
      "epoch: 55\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0394\n",
      "epoch: 56\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0417\n",
      "epoch: 57\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0402\n",
      "epoch: 58\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0404\n",
      "epoch: 59\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0406\n",
      "epoch: 60\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0373\n",
      "epoch: 61\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0406\n",
      "epoch: 62\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0402\n",
      "epoch: 63\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0427\n",
      "epoch: 64\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0391\n",
      "epoch: 65\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0405\n",
      "epoch: 66\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0368\n",
      "epoch: 67\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0386\n",
      "epoch: 68\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 28ms/sample - loss: 0.0402\n",
      "epoch: 69\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0352\n",
      "epoch: 70\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0355\n",
      "epoch: 71\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0388\n",
      "epoch: 72\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 34ms/sample - loss: 0.0404\n",
      "epoch: 73\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 35ms/sample - loss: 0.0374\n",
      "epoch: 74\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0410\n",
      "epoch: 75\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0406\n",
      "epoch: 76\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0380\n",
      "epoch: 77\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 7s 36ms/sample - loss: 0.0369\n",
      "epoch: 78\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0377\n",
      "epoch: 79\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0340\n",
      "epoch: 80\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0333\n",
      "epoch: 81\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0393\n",
      "epoch: 82\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0353\n",
      "epoch: 83\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0384\n",
      "epoch: 84\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0400\n",
      "epoch: 85\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0341\n",
      "epoch: 86\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0392\n",
      "epoch: 87\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0435\n",
      "epoch: 88\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0379\n",
      "epoch: 89\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0390\n",
      "epoch: 90\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0352\n",
      "epoch: 91\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0366\n",
      "epoch: 92\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0394\n",
      "epoch: 93\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0364\n",
      "epoch: 94\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0400\n",
      "epoch: 95\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 29ms/sample - loss: 0.0381\n",
      "epoch: 96\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 5s 30ms/sample - loss: 0.0357\n",
      "epoch: 97\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0365\n",
      "epoch: 98\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 31ms/sample - loss: 0.0370\n",
      "epoch: 99\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 32ms/sample - loss: 0.0353\n",
      "epoch: 100\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 6s 33ms/sample - loss: 0.0402\n",
      "Forecasting Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-04-01 17:40:50.920781: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=10.296289, Expected=9.510000\n",
      "Month=2, Predicted=9.811337, Expected=9.796000\n",
      "Month=3, Predicted=9.654682, Expected=9.468500\n",
      "Month=4, Predicted=9.551023, Expected=9.672000\n",
      "Month=5, Predicted=9.704791, Expected=9.610000\n",
      "Month=6, Predicted=9.562144, Expected=9.240000\n",
      "Month=7, Predicted=9.446524, Expected=10.318300\n",
      "Month=8, Predicted=9.873698, Expected=8.974800\n",
      "Month=9, Predicted=9.490310, Expected=9.114000\n",
      "Month=10, Predicted=9.103058, Expected=9.300000\n",
      "Month=11, Predicted=9.016076, Expected=8.400000\n",
      "Month=12, Predicted=8.914551, Expected=9.300000\n",
      "Month=13, Predicted=8.893553, Expected=9.000000\n",
      "Month=14, Predicted=9.063683, Expected=9.300000\n",
      "Month=15, Predicted=9.343415, Expected=9.460000\n",
      "Month=16, Predicted=9.267518, Expected=9.145000\n",
      "Month=17, Predicted=9.350873, Expected=9.021000\n",
      "Month=18, Predicted=9.022127, Expected=8.750000\n",
      "Month=19, Predicted=8.815897, Expected=8.710000\n",
      "Month=20, Predicted=8.757490, Expected=8.370000\n",
      "Month=21, Predicted=8.489883, Expected=8.504000\n",
      "Month=22, Predicted=8.471028, Expected=9.819700\n",
      "Month=23, Predicted=9.343528, Expected=9.827300\n",
      "Month=24, Predicted=9.921607, Expected=9.929800\n",
      "Month=25, Predicted=9.922395, Expected=9.288000\n",
      "Month=26, Predicted=9.426569, Expected=9.300000\n",
      "Month=27, Predicted=9.296190, Expected=9.060000\n",
      "Month=28, Predicted=9.097182, Expected=8.835000\n",
      "Month=29, Predicted=9.002695, Expected=8.388600\n",
      "Month=30, Predicted=8.512420, Expected=8.400000\n",
      "Month=31, Predicted=8.395423, Expected=8.525000\n",
      "Month=32, Predicted=8.473922, Expected=8.250000\n",
      "Month=33, Predicted=8.419248, Expected=8.419000\n",
      "Month=34, Predicted=8.337589, Expected=9.455000\n",
      "Month=35, Predicted=9.058847, Expected=8.540000\n",
      "Month=36, Predicted=8.970915, Expected=9.455000\n",
      "Month=37, Predicted=9.094399, Expected=9.000000\n",
      "Month=38, Predicted=9.047141, Expected=9.599000\n",
      "Month=39, Predicted=9.537613, Expected=9.436000\n",
      "Month=40, Predicted=9.367549, Expected=9.539800\n",
      "Month=41, Predicted=9.608039, Expected=9.028600\n",
      "Month=42, Predicted=9.135965, Expected=8.932000\n",
      "Month=43, Predicted=8.991596, Expected=8.993000\n",
      "Month=44, Predicted=8.914208, Expected=8.678400\n",
      "Month=45, Predicted=8.870305, Expected=9.011100\n",
      "Month=46, Predicted=8.874638, Expected=9.630000\n",
      "Month=47, Predicted=9.385109, Expected=8.590400\n",
      "Month=48, Predicted=9.063070, Expected=9.736300\n",
      "Month=49, Predicted=9.258750, Expected=9.384500\n",
      "Month=50, Predicted=9.425950, Expected=9.947200\n",
      "Month=51, Predicted=9.918702, Expected=9.577100\n",
      "Month=52, Predicted=9.574021, Expected=9.117200\n",
      "Month=53, Predicted=9.373135, Expected=9.122500\n",
      "Month=54, Predicted=9.000302, Expected=8.880000\n",
      "Month=55, Predicted=9.005130, Expected=8.709200\n",
      "Month=56, Predicted=8.819741, Expected=8.428200\n",
      "Month=57, Predicted=8.497756, Expected=9.907600\n",
      "Month=58, Predicted=9.402804, Expected=9.145000\n",
      "Month=59, Predicted=9.479354, Expected=8.498000\n",
      "Month=60, Predicted=8.790590, Expected=9.362000\n",
      "Month=61, Predicted=8.834273, Expected=9.000000\n",
      "Month=62, Predicted=9.232922, Expected=9.455000\n",
      "Month=63, Predicted=9.376235, Expected=9.300000\n",
      "Month=64, Predicted=9.237703, Expected=8.990000\n",
      "Month=65, Predicted=9.198203, Expected=8.990000\n",
      "Month=66, Predicted=8.902480, Expected=8.790000\n",
      "Month=67, Predicted=8.879742, Expected=8.835000\n",
      "Month=68, Predicted=8.851939, Expected=8.700000\n",
      "Month=69, Predicted=8.733765, Expected=8.935000\n",
      "Month=70, Predicted=8.873649, Expected=8.835000\n",
      "Month=71, Predicted=8.867827, Expected=8.265000\n",
      "Month=72, Predicted=8.513327, Expected=8.835000\n",
      "Month=73, Predicted=8.556327, Expected=8.550000\n",
      "Month=74, Predicted=8.665226, Expected=8.680000\n",
      "Month=75, Predicted=8.727687, Expected=8.400000\n",
      "Month=76, Predicted=8.408183, Expected=8.525000\n",
      "Month=77, Predicted=8.529237, Expected=8.370000\n",
      "Month=78, Predicted=8.395233, Expected=7.890000\n",
      "Month=79, Predicted=8.117472, Expected=7.812000\n",
      "Month=80, Predicted=7.787504, Expected=7.620000\n",
      "Month=81, Predicted=7.683157, Expected=7.718000\n",
      "Month=82, Predicted=7.732031, Expected=8.323500\n",
      "Month=83, Predicted=8.087186, Expected=6.860000\n",
      "Month=84, Predicted=7.432370, Expected=8.308000\n",
      "Month=85, Predicted=7.749218, Expected=8.100000\n",
      "Month=86, Predicted=8.060998, Expected=8.525000\n",
      "Month=87, Predicted=8.591964, Expected=8.250000\n",
      "Month=88, Predicted=8.191751, Expected=8.215000\n",
      "Month=89, Predicted=8.289035, Expected=8.122600\n",
      "Month=90, Predicted=8.085137, Expected=7.778100\n",
      "Month=91, Predicted=7.954215, Expected=7.954600\n",
      "Month=92, Predicted=7.865444, Expected=7.420000\n",
      "Month=93, Predicted=7.621394, Expected=7.538300\n",
      "Month=94, Predicted=7.528245, Expected=7.905000\n",
      "Month=95, Predicted=7.702186, Expected=7.140000\n",
      "Month=96, Predicted=7.527240, Expected=8.432000\n",
      "Month=97, Predicted=7.926647, Expected=7.710000\n",
      "Month=98, Predicted=7.935185, Expected=7.967000\n",
      "Month=99, Predicted=8.005265, Expected=7.320000\n",
      "Month=100, Predicted=7.378392, Expected=7.502000\n",
      "Month=101, Predicted=7.535028, Expected=7.409000\n",
      "Month=102, Predicted=7.355297, Expected=7.200600\n",
      "Month=103, Predicted=7.383208, Expected=7.865000\n",
      "Month=104, Predicted=7.560789, Expected=6.690000\n",
      "Month=105, Predicted=7.151611, Expected=6.879400\n",
      "Month=106, Predicted=6.821202, Expected=7.440000\n",
      "Month=107, Predicted=7.072918, Expected=6.860000\n",
      "Month=108, Predicted=7.263651, Expected=7.595000\n",
      "Month=109, Predicted=7.273226, Expected=7.200000\n",
      "Month=110, Predicted=7.280295, Expected=7.130000\n",
      "Month=111, Predicted=7.275848, Expected=6.900000\n",
      "Month=112, Predicted=6.855963, Expected=7.130000\n",
      "Month=113, Predicted=7.098688, Expected=7.130000\n",
      "Month=114, Predicted=7.128191, Expected=6.840000\n",
      "Month=115, Predicted=6.997642, Expected=7.006000\n",
      "Month=116, Predicted=6.895978, Expected=6.780000\n",
      "Month=117, Predicted=6.864561, Expected=7.089600\n",
      "Month=118, Predicted=7.016570, Expected=6.882000\n",
      "Month=119, Predicted=6.933017, Expected=6.446700\n",
      "Month=120, Predicted=6.660321, Expected=6.882000\n",
      "Month=121, Predicted=6.639946, Expected=6.600000\n",
      "Month=122, Predicted=6.735200, Expected=6.820000\n",
      "Month=123, Predicted=6.806264, Expected=6.600000\n",
      "Month=124, Predicted=6.608924, Expected=6.820000\n",
      "Month=125, Predicted=6.793926, Expected=6.665000\n",
      "Month=126, Predicted=6.687235, Expected=6.450000\n",
      "Month=127, Predicted=6.578967, Expected=6.665000\n",
      "Month=128, Predicted=6.531115, Expected=6.450000\n",
      "Month=129, Predicted=6.558391, Expected=6.722100\n",
      "Month=130, Predicted=6.651414, Expected=6.820000\n",
      "Month=131, Predicted=6.749242, Expected=6.160000\n",
      "Month=132, Predicted=6.468080, Expected=6.820000\n",
      "Month=133, Predicted=6.510589, Expected=6.480000\n",
      "Month=134, Predicted=6.586958, Expected=6.596900\n",
      "Month=135, Predicted=6.668615, Expected=6.492000\n",
      "Month=136, Predicted=6.416629, Expected=6.510000\n",
      "Month=137, Predicted=6.570628, Expected=6.339500\n",
      "Month=138, Predicted=6.378390, Expected=6.001600\n",
      "Month=139, Predicted=6.144545, Expected=6.107000\n",
      "Month=140, Predicted=6.038145, Expected=5.790000\n",
      "Month=141, Predicted=5.919923, Expected=5.885000\n",
      "Month=142, Predicted=5.886220, Expected=7.280000\n",
      "Month=143, Predicted=6.759653, Expected=5.941600\n",
      "Month=144, Predicted=6.517357, Expected=6.810000\n",
      "Month=145, Predicted=6.464393, Expected=6.182000\n",
      "Month=146, Predicted=6.240187, Expected=6.293000\n",
      "Month=147, Predicted=6.447945, Expected=6.118600\n",
      "Month=148, Predicted=6.014861, Expected=6.138000\n",
      "Month=149, Predicted=6.231922, Expected=6.107000\n",
      "Month=150, Predicted=6.080102, Expected=5.913000\n",
      "Month=151, Predicted=6.017163, Expected=6.141100\n",
      "Month=152, Predicted=6.039951, Expected=6.248000\n",
      "Month=153, Predicted=6.213408, Expected=5.829700\n",
      "Month=154, Predicted=6.038184, Expected=6.829300\n",
      "Month=155, Predicted=6.418721, Expected=6.694400\n",
      "Month=156, Predicted=6.751206, Expected=7.726200\n",
      "Month=157, Predicted=7.452864, Expected=7.054400\n",
      "Month=158, Predicted=7.228696, Expected=7.268900\n",
      "Month=159, Predicted=7.247927, Expected=7.020000\n",
      "Month=160, Predicted=6.970500, Expected=6.510000\n",
      "Month=161, Predicted=6.812690, Expected=6.370500\n",
      "Month=162, Predicted=6.338369, Expected=5.730000\n",
      "Month=163, Predicted=5.968298, Expected=5.828000\n",
      "Month=164, Predicted=5.817945, Expected=5.580000\n",
      "Month=165, Predicted=5.638150, Expected=5.709900\n",
      "Month=166, Predicted=5.740615, Expected=6.696000\n",
      "Month=167, Predicted=6.299580, Expected=6.248000\n",
      "Month=168, Predicted=6.501753, Expected=6.711600\n",
      "Month=169, Predicted=6.541825, Expected=6.600100\n",
      "Month=170, Predicted=6.537710, Expected=7.508200\n",
      "Month=171, Predicted=7.276092, Expected=7.765000\n",
      "Month=172, Predicted=7.632943, Expected=7.285000\n",
      "Month=173, Predicted=7.553619, Expected=6.959500\n",
      "Month=174, Predicted=6.982373, Expected=6.450000\n",
      "Month=175, Predicted=6.581735, Expected=6.572000\n",
      "Month=176, Predicted=6.557161, Expected=6.600000\n",
      "Month=177, Predicted=6.590821, Expected=4.265300\n",
      "Month=178, Predicted=5.037923, Expected=7.367000\n",
      "Month=179, Predicted=6.379995, Expected=6.544000\n",
      "Month=180, Predicted=6.753733, Expected=6.940800\n",
      "Train RMSE: 0.4974\n",
      "Train RMSPE: 7.2310\n",
      "Train MAE: 0.34433\n",
      "Train MAPE: 4.55157\n",
      "Forecasting Testing Data\n",
      "Month=1, Predicted=6.996511, Expected=6.786000\n",
      "Month=2, Predicted=6.610583, Expected=6.981200\n",
      "Month=3, Predicted=7.028344, Expected=6.756000\n",
      "Month=4, Predicted=6.772181, Expected=6.733200\n",
      "Month=5, Predicted=6.778630, Expected=6.671200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=6, Predicted=6.653432, Expected=6.295600\n",
      "Month=7, Predicted=6.474430, Expected=6.432500\n",
      "Month=8, Predicted=6.358804, Expected=6.153000\n",
      "Month=9, Predicted=6.250873, Expected=6.389500\n",
      "Month=10, Predicted=6.349254, Expected=7.192000\n",
      "Month=11, Predicted=6.862939, Expected=6.524000\n",
      "Month=12, Predicted=6.865702, Expected=7.238500\n",
      "Month=13, Predicted=6.945328, Expected=6.990000\n",
      "Month=14, Predicted=6.989115, Expected=7.254000\n",
      "Month=15, Predicted=7.291464, Expected=6.720000\n",
      "Month=16, Predicted=6.817353, Expected=6.944000\n",
      "Month=17, Predicted=6.902719, Expected=7.052500\n",
      "Month=18, Predicted=6.946389, Expected=6.690000\n",
      "Month=19, Predicted=6.928862, Expected=6.909900\n",
      "Month=20, Predicted=6.773693, Expected=6.819000\n",
      "Month=21, Predicted=6.833294, Expected=7.167200\n",
      "Month=22, Predicted=7.102824, Expected=7.254000\n",
      "Month=23, Predicted=7.194662, Expected=6.664000\n",
      "Month=24, Predicted=6.933880, Expected=7.393500\n",
      "Month=25, Predicted=7.057339, Expected=7.125000\n",
      "Month=26, Predicted=7.216107, Expected=7.347000\n",
      "Month=27, Predicted=7.375472, Expected=7.216500\n",
      "Month=28, Predicted=7.156947, Expected=7.254000\n",
      "Month=29, Predicted=7.303982, Expected=7.238500\n",
      "Month=30, Predicted=7.206036, Expected=6.990000\n",
      "Month=31, Predicted=7.114483, Expected=7.192000\n",
      "Month=32, Predicted=7.098017, Expected=6.900000\n",
      "Month=33, Predicted=7.011032, Expected=7.427300\n",
      "Month=34, Predicted=7.264655, Expected=7.300500\n",
      "Month=35, Predicted=7.322200, Expected=6.902000\n",
      "Month=36, Predicted=7.126962, Expected=7.409000\n",
      "Month=37, Predicted=7.121496, Expected=7.179000\n",
      "Month=38, Predicted=7.292803, Expected=7.424500\n",
      "Month=39, Predicted=7.405248, Expected=7.275000\n",
      "Month=40, Predicted=7.255244, Expected=7.316000\n",
      "Month=41, Predicted=7.357495, Expected=7.086300\n",
      "Month=42, Predicted=7.132359, Expected=7.020000\n",
      "Month=43, Predicted=7.068584, Expected=7.270500\n",
      "Month=44, Predicted=7.150638, Expected=7.168800\n",
      "Month=45, Predicted=7.251536, Expected=7.448600\n",
      "Month=46, Predicted=7.359232, Expected=7.440200\n",
      "Test RMSE: 0.2700\n",
      "Test RMSPE: 3.8679\n",
      "Test MAE: 0.21043\n",
      "Test MAPE: 3.01247\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# multi rnn \n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import random as rn\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(SimpleRNN(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(SimpleRNN(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\tmodel.add(SimpleRNN(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[3], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\t\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('epoch:',i+1)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\n",
    "\treturn model\n",
    "\t\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\t\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\t\n",
    "\t\n",
    "\n",
    "\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\n",
    "\t\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.4f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.4f' % rmspe_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.4f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_test = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "\tprint('Test RMSPE: %.4f' % rmspe_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel('Time',fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\n",
    "\t \n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv(\"oil_production.csv\", header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back= 5\n",
    "\tneurons= [ 4 , 3 , 4 ]\n",
    "\tn_epoch=100\n",
    "\texperiment(series,look_back,neurons,n_epoch)\n",
    "\n",
    "run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4e4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "Train on 180 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:42:33.320784: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-01 17:42:33.321065: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/3700061504.py:29: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  from pandas import datetime\n",
      "/var/folders/pb/zjk7bcqn4l73lc2cmv6y5_gh0000gn/T/ipykernel_8441/3700061504.py:238: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  series = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
      "2022-04-01 17:42:33.595598: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:42:33.652133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-04-01 17:42:33.794403: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 3s 11ms/sample - loss: 0.2500\n",
      "epoch: 2\n",
      "Train on 180 samples\n",
      " 15/180 [=>............................] - ETA: 1s - loss: 0.1787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-01 17:42:36.116854: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180/180 [==============================] - 2s 11ms/sample - loss: 0.1377\n",
      "epoch: 3\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.1088\n",
      "epoch: 4\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.1137\n",
      "epoch: 5\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0926\n",
      "epoch: 6\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0818\n",
      "epoch: 7\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0689\n",
      "epoch: 8\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0553\n",
      "epoch: 9\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0663\n",
      "epoch: 10\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0524\n",
      "epoch: 11\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0499\n",
      "epoch: 12\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0476\n",
      "epoch: 13\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0456\n",
      "epoch: 14\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0543\n",
      "epoch: 15\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0416\n",
      "epoch: 16\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0471\n",
      "epoch: 17\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0431\n",
      "epoch: 18\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0409\n",
      "epoch: 19\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0397\n",
      "epoch: 20\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0386\n",
      "epoch: 21\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0447\n",
      "epoch: 22\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 11ms/sample - loss: 0.0391\n",
      "epoch: 23\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0392\n",
      "epoch: 24\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0425\n",
      "epoch: 25\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0365\n",
      "epoch: 26\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0376\n",
      "epoch: 27\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0386\n",
      "epoch: 28\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0352\n",
      "epoch: 29\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0358\n",
      "epoch: 30\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0353\n",
      "epoch: 31\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0343\n",
      "epoch: 32\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0336\n",
      "epoch: 33\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 34\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0412\n",
      "epoch: 35\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 36\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0337\n",
      "epoch: 37\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0345\n",
      "epoch: 38\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0365\n",
      "epoch: 39\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0320\n",
      "epoch: 40\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0327\n",
      "epoch: 41\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0371\n",
      "epoch: 42\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 43\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0345\n",
      "epoch: 44\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0365\n",
      "epoch: 45\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0387\n",
      "epoch: 46\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0415\n",
      "epoch: 47\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0340\n",
      "epoch: 48\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0333\n",
      "epoch: 49\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0362\n",
      "epoch: 50\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 51\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0331\n",
      "epoch: 52\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0331\n",
      "epoch: 53\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0343\n",
      "epoch: 54\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0330\n",
      "epoch: 55\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0335\n",
      "epoch: 56\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0354\n",
      "epoch: 57\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0374\n",
      "epoch: 58\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0383\n",
      "epoch: 59\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0326\n",
      "epoch: 60\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0338\n",
      "epoch: 61\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0306\n",
      "epoch: 62\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0311\n",
      "epoch: 63\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0392\n",
      "epoch: 64\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0373\n",
      "epoch: 65\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0352\n",
      "epoch: 66\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0337\n",
      "epoch: 67\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0380\n",
      "epoch: 68\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0332\n",
      "epoch: 69\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0354\n",
      "epoch: 70\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0322\n",
      "epoch: 71\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0374\n",
      "epoch: 72\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0365\n",
      "epoch: 73\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0395\n",
      "epoch: 74\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0347\n",
      "epoch: 75\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0324\n",
      "epoch: 76\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0349\n",
      "epoch: 77\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0323\n",
      "epoch: 78\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 79\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 80\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0360\n",
      "epoch: 81\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0319\n",
      "epoch: 82\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0393\n",
      "epoch: 83\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 9ms/sample - loss: 0.0341\n",
      "epoch: 84\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0332\n",
      "epoch: 85\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0329\n",
      "epoch: 86\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0326\n",
      "epoch: 87\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0383\n",
      "epoch: 88\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0331\n",
      "epoch: 89\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0330\n",
      "epoch: 90\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0338\n",
      "epoch: 91\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0329\n",
      "epoch: 92\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0334\n",
      "epoch: 93\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0375\n",
      "epoch: 94\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0380\n",
      "epoch: 95\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0371\n",
      "epoch: 96\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0342\n",
      "epoch: 97\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0346\n",
      "epoch: 98\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0359\n",
      "epoch: 99\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0377\n",
      "epoch: 100\n",
      "Train on 180 samples\n",
      "180/180 [==============================] - 2s 10ms/sample - loss: 0.0346\n",
      "Forecasting Training Data\n",
      "Month=1, Predicted=10.294142, Expected=9.510000\n",
      "Month=2, Predicted=9.873382, Expected=9.796000\n",
      "Month=3, Predicted=9.648378, Expected=9.468500\n",
      "Month=4, Predicted=9.562604, Expected=9.672000\n",
      "Month=5, Predicted=9.695996, Expected=9.610000\n",
      "Month=6, Predicted=9.606192, Expected=9.240000\n",
      "Month=7, Predicted=9.464187, Expected=10.318300\n",
      "Month=8, Predicted=9.763935, Expected=8.974800\n",
      "Month=9, Predicted=9.544981, Expected=9.114000\n",
      "Month=10, Predicted=9.133076, Expected=9.300000\n",
      "Month=11, Predicted=9.016662, Expected=8.400000\n",
      "Month=12, Predicted=8.954903, Expected=9.300000\n",
      "Month=13, Predicted=8.906993, Expected=9.000000\n",
      "Month=14, Predicted=9.045565, Expected=9.300000\n",
      "Month=15, Predicted=9.296061, Expected=9.460000\n",
      "Month=16, Predicted=9.266897, Expected=9.145000\n",
      "Month=17, Predicted=9.286334, Expected=9.021000\n",
      "Month=18, Predicted=9.016746, Expected=8.750000\n",
      "Month=19, Predicted=8.798419, Expected=8.710000\n",
      "Month=20, Predicted=8.738564, Expected=8.370000\n",
      "Month=21, Predicted=8.540072, Expected=8.504000\n",
      "Month=22, Predicted=8.486552, Expected=9.819700\n",
      "Month=23, Predicted=9.186067, Expected=9.827300\n",
      "Month=24, Predicted=9.816735, Expected=9.929800\n",
      "Month=25, Predicted=9.876760, Expected=9.288000\n",
      "Month=26, Predicted=9.441103, Expected=9.300000\n",
      "Month=27, Predicted=9.218135, Expected=9.060000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaitanya_kr_01/tensorflow-test/env/lib/python3.8/site-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "2022-04-01 17:45:28.615288: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=28, Predicted=9.078411, Expected=8.835000\n",
      "Month=29, Predicted=9.019611, Expected=8.388600\n",
      "Month=30, Predicted=8.621252, Expected=8.400000\n",
      "Month=31, Predicted=8.435446, Expected=8.525000\n",
      "Month=32, Predicted=8.484918, Expected=8.250000\n",
      "Month=33, Predicted=8.465146, Expected=8.419000\n",
      "Month=34, Predicted=8.367053, Expected=9.455000\n",
      "Month=35, Predicted=8.909649, Expected=8.540000\n",
      "Month=36, Predicted=8.960261, Expected=9.455000\n",
      "Month=37, Predicted=9.008789, Expected=9.000000\n",
      "Month=38, Predicted=9.017806, Expected=9.599000\n",
      "Month=39, Predicted=9.398024, Expected=9.436000\n",
      "Month=40, Predicted=9.370504, Expected=9.539800\n",
      "Month=41, Predicted=9.513702, Expected=9.028600\n",
      "Month=42, Predicted=9.162941, Expected=8.932000\n",
      "Month=43, Predicted=8.960216, Expected=8.993000\n",
      "Month=44, Predicted=8.895527, Expected=8.678400\n",
      "Month=45, Predicted=8.882563, Expected=9.011100\n",
      "Month=46, Predicted=8.885351, Expected=9.630000\n",
      "Month=47, Predicted=9.288845, Expected=8.590400\n",
      "Month=48, Predicted=9.107350, Expected=9.736300\n",
      "Month=49, Predicted=9.154176, Expected=9.384500\n",
      "Month=50, Predicted=9.355659, Expected=9.947200\n",
      "Month=51, Predicted=9.795505, Expected=9.577100\n",
      "Month=52, Predicted=9.618071, Expected=9.117200\n",
      "Month=53, Predicted=9.353692, Expected=9.122500\n",
      "Month=54, Predicted=9.005335, Expected=8.880000\n",
      "Month=55, Predicted=8.982365, Expected=8.709200\n",
      "Month=56, Predicted=8.867648, Expected=8.428200\n",
      "Month=57, Predicted=8.581182, Expected=9.907600\n",
      "Month=58, Predicted=9.240537, Expected=9.145000\n",
      "Month=59, Predicted=9.477642, Expected=8.498000\n",
      "Month=60, Predicted=8.904972, Expected=9.362000\n",
      "Month=61, Predicted=8.767317, Expected=9.000000\n",
      "Month=62, Predicted=9.152130, Expected=9.455000\n",
      "Month=63, Predicted=9.375069, Expected=9.300000\n",
      "Month=64, Predicted=9.262099, Expected=8.990000\n",
      "Month=65, Predicted=9.157601, Expected=8.990000\n",
      "Month=66, Predicted=8.898311, Expected=8.790000\n",
      "Month=67, Predicted=8.846455, Expected=8.835000\n",
      "Month=68, Predicted=8.843991, Expected=8.700000\n",
      "Month=69, Predicted=8.755407, Expected=8.935000\n",
      "Month=70, Predicted=8.841848, Expected=8.835000\n",
      "Month=71, Predicted=8.851908, Expected=8.265000\n",
      "Month=72, Predicted=8.551379, Expected=8.835000\n",
      "Month=73, Predicted=8.505806, Expected=8.550000\n",
      "Month=74, Predicted=8.621853, Expected=8.680000\n",
      "Month=75, Predicted=8.711857, Expected=8.400000\n",
      "Month=76, Predicted=8.449001, Expected=8.525000\n",
      "Month=77, Predicted=8.471952, Expected=8.370000\n",
      "Month=78, Predicted=8.388839, Expected=7.890000\n",
      "Month=79, Predicted=8.149934, Expected=7.812000\n",
      "Month=80, Predicted=7.827663, Expected=7.620000\n",
      "Month=81, Predicted=7.698542, Expected=7.718000\n",
      "Month=82, Predicted=7.741438, Expected=8.323500\n",
      "Month=83, Predicted=8.045922, Expected=6.860000\n",
      "Month=84, Predicted=7.565956, Expected=8.308000\n",
      "Month=85, Predicted=7.627580, Expected=8.100000\n",
      "Month=86, Predicted=7.992839, Expected=8.525000\n",
      "Month=87, Predicted=8.495153, Expected=8.250000\n",
      "Month=88, Predicted=8.262591, Expected=8.215000\n",
      "Month=89, Predicted=8.208791, Expected=8.122600\n",
      "Month=90, Predicted=8.052090, Expected=7.778100\n",
      "Month=91, Predicted=7.934298, Expected=7.954600\n",
      "Month=92, Predicted=7.857739, Expected=7.420000\n",
      "Month=93, Predicted=7.659683, Expected=7.538300\n",
      "Month=94, Predicted=7.544489, Expected=7.905000\n",
      "Month=95, Predicted=7.672784, Expected=7.140000\n",
      "Month=96, Predicted=7.570474, Expected=8.432000\n",
      "Month=97, Predicted=7.828854, Expected=7.710000\n",
      "Month=98, Predicted=7.914776, Expected=7.967000\n",
      "Month=99, Predicted=7.959394, Expected=7.320000\n",
      "Month=100, Predicted=7.475758, Expected=7.502000\n",
      "Month=101, Predicted=7.476382, Expected=7.409000\n",
      "Month=102, Predicted=7.381105, Expected=7.200600\n",
      "Month=103, Predicted=7.393481, Expected=7.865000\n",
      "Month=104, Predicted=7.528374, Expected=6.690000\n",
      "Month=105, Predicted=7.219954, Expected=6.879400\n",
      "Month=106, Predicted=6.844292, Expected=7.440000\n",
      "Month=107, Predicted=7.005574, Expected=6.860000\n",
      "Month=108, Predicted=7.254350, Expected=7.595000\n",
      "Month=109, Predicted=7.275390, Expected=7.200000\n",
      "Month=110, Predicted=7.254250, Expected=7.130000\n",
      "Month=111, Predicted=7.228677, Expected=6.900000\n",
      "Month=112, Predicted=6.890225, Expected=7.130000\n",
      "Month=113, Predicted=7.022092, Expected=7.130000\n",
      "Month=114, Predicted=7.108167, Expected=6.840000\n",
      "Month=115, Predicted=7.013769, Expected=7.006000\n",
      "Month=116, Predicted=6.883693, Expected=6.780000\n",
      "Month=117, Predicted=6.835330, Expected=7.089600\n",
      "Month=118, Predicted=6.963724, Expected=6.882000\n",
      "Month=119, Predicted=6.926436, Expected=6.446700\n",
      "Month=120, Predicted=6.680555, Expected=6.882000\n",
      "Month=121, Predicted=6.603241, Expected=6.600000\n",
      "Month=122, Predicted=6.696253, Expected=6.820000\n",
      "Month=123, Predicted=6.788117, Expected=6.600000\n",
      "Month=124, Predicted=6.628288, Expected=6.820000\n",
      "Month=125, Predicted=6.725163, Expected=6.665000\n",
      "Month=126, Predicted=6.668914, Expected=6.450000\n",
      "Month=127, Predicted=6.562980, Expected=6.665000\n",
      "Month=128, Predicted=6.501205, Expected=6.450000\n",
      "Month=129, Predicted=6.524774, Expected=6.722100\n",
      "Month=130, Predicted=6.613719, Expected=6.820000\n",
      "Month=131, Predicted=6.705179, Expected=6.160000\n",
      "Month=132, Predicted=6.483405, Expected=6.820000\n",
      "Month=133, Predicted=6.447806, Expected=6.480000\n",
      "Month=134, Predicted=6.535866, Expected=6.596900\n",
      "Month=135, Predicted=6.635212, Expected=6.492000\n",
      "Month=136, Predicted=6.440500, Expected=6.510000\n",
      "Month=137, Predicted=6.507750, Expected=6.339500\n",
      "Month=138, Predicted=6.379098, Expected=6.001600\n",
      "Month=139, Predicted=6.158870, Expected=6.107000\n",
      "Month=140, Predicted=6.032066, Expected=5.790000\n",
      "Month=141, Predicted=5.933896, Expected=5.885000\n",
      "Month=142, Predicted=5.897979, Expected=7.280000\n",
      "Month=143, Predicted=6.593210, Expected=5.941600\n",
      "Month=144, Predicted=6.563002, Expected=6.810000\n",
      "Month=145, Predicted=6.418306, Expected=6.182000\n",
      "Month=146, Predicted=6.272154, Expected=6.293000\n",
      "Month=147, Predicted=6.378979, Expected=6.118600\n",
      "Month=148, Predicted=6.087057, Expected=6.138000\n",
      "Month=149, Predicted=6.186711, Expected=6.107000\n",
      "Month=150, Predicted=6.099134, Expected=5.913000\n",
      "Month=151, Predicted=6.019085, Expected=6.141100\n",
      "Month=152, Predicted=6.014067, Expected=6.248000\n",
      "Month=153, Predicted=6.158390, Expected=5.829700\n",
      "Month=154, Predicted=6.043971, Expected=6.829300\n",
      "Month=155, Predicted=6.302915, Expected=6.694400\n",
      "Month=156, Predicted=6.644616, Expected=7.726200\n",
      "Month=157, Predicted=7.279000, Expected=7.054400\n",
      "Month=158, Predicted=7.225245, Expected=7.268900\n",
      "Month=159, Predicted=7.149540, Expected=7.020000\n",
      "Month=160, Predicted=6.951894, Expected=6.510000\n",
      "Month=161, Predicted=6.797320, Expected=6.370500\n",
      "Month=162, Predicted=6.404063, Expected=5.730000\n",
      "Month=163, Predicted=6.041693, Expected=5.828000\n",
      "Month=164, Predicted=5.867903, Expected=5.580000\n",
      "Month=165, Predicted=5.718140, Expected=5.709900\n",
      "Month=166, Predicted=5.770717, Expected=6.696000\n",
      "Month=167, Predicted=6.222541, Expected=6.248000\n",
      "Month=168, Predicted=6.461791, Expected=6.711600\n",
      "Month=169, Predicted=6.484046, Expected=6.600100\n",
      "Month=170, Predicted=6.477065, Expected=7.508200\n",
      "Month=171, Predicted=7.069042, Expected=7.765000\n",
      "Month=172, Predicted=7.507020, Expected=7.285000\n",
      "Month=173, Predicted=7.485246, Expected=6.959500\n",
      "Month=174, Predicted=6.980776, Expected=6.450000\n",
      "Month=175, Predicted=6.565897, Expected=6.572000\n",
      "Month=176, Predicted=6.527332, Expected=6.600000\n",
      "Month=177, Predicted=6.612646, Expected=4.265300\n",
      "Month=178, Predicted=5.367749, Expected=7.367000\n",
      "Month=179, Predicted=6.040545, Expected=6.544000\n",
      "Month=180, Predicted=6.765397, Expected=6.940800\n",
      "Train RMSE: 0.4821\n",
      "Train RMSPE: 7.0642\n",
      "Train MAE: 0.33480\n",
      "Train MAPE: 4.42587\n",
      "Forecasting Testing Data\n",
      "Month=1, Predicted=6.987675, Expected=6.786000\n",
      "Month=2, Predicted=6.798150, Expected=6.981200\n",
      "Month=3, Predicted=6.953904, Expected=6.756000\n",
      "Month=4, Predicted=6.793846, Expected=6.733200\n",
      "Month=5, Predicted=6.739552, Expected=6.671200\n",
      "Month=6, Predicted=6.633174, Expected=6.295600\n",
      "Month=7, Predicted=6.478112, Expected=6.432500\n",
      "Month=8, Predicted=6.359928, Expected=6.153000\n",
      "Month=9, Predicted=6.262577, Expected=6.389500\n",
      "Month=10, Predicted=6.331725, Expected=7.192000\n",
      "Month=11, Predicted=6.763225, Expected=6.524000\n",
      "Month=12, Predicted=6.844758, Expected=7.238500\n",
      "Month=13, Predicted=6.869964, Expected=6.990000\n",
      "Month=14, Predicted=6.924937, Expected=7.254000\n",
      "Month=15, Predicted=7.175922, Expected=6.720000\n",
      "Month=16, Predicted=6.857350, Expected=6.944000\n",
      "Month=17, Predicted=6.831553, Expected=7.052500\n",
      "Month=18, Predicted=6.897850, Expected=6.690000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=19, Predicted=6.917344, Expected=6.909900\n",
      "Month=20, Predicted=6.778826, Expected=6.819000\n",
      "Month=21, Predicted=6.790933, Expected=7.167200\n",
      "Month=22, Predicted=7.025213, Expected=7.254000\n",
      "Month=23, Predicted=7.147489, Expected=6.664000\n",
      "Month=24, Predicted=6.933524, Expected=7.393500\n",
      "Month=25, Predicted=6.963945, Expected=7.125000\n",
      "Month=26, Predicted=7.136551, Expected=7.347000\n",
      "Month=27, Predicted=7.315993, Expected=7.216500\n",
      "Month=28, Predicted=7.162850, Expected=7.254000\n",
      "Month=29, Predicted=7.224631, Expected=7.238500\n",
      "Month=30, Predicted=7.172372, Expected=6.990000\n",
      "Month=31, Predicted=7.089445, Expected=7.192000\n",
      "Month=32, Predicted=7.062775, Expected=6.900000\n",
      "Month=33, Predicted=6.998045, Expected=7.427300\n",
      "Month=34, Predicted=7.194997, Expected=7.300500\n",
      "Month=35, Predicted=7.286190, Expected=6.902000\n",
      "Month=36, Predicted=7.126551, Expected=7.409000\n",
      "Month=37, Predicted=7.066402, Expected=7.179000\n",
      "Month=38, Predicted=7.217467, Expected=7.424500\n",
      "Month=39, Predicted=7.357315, Expected=7.275000\n",
      "Month=40, Predicted=7.249666, Expected=7.316000\n",
      "Month=41, Predicted=7.288865, Expected=7.086300\n",
      "Month=42, Predicted=7.123614, Expected=7.020000\n",
      "Month=43, Predicted=7.039380, Expected=7.270500\n",
      "Month=44, Predicted=7.105744, Expected=7.168800\n",
      "Month=45, Predicted=7.215921, Expected=7.448600\n",
      "Month=46, Predicted=7.316894, Expected=7.440200\n",
      "Test RMSE: 0.2616\n",
      "Test RMSPE: 3.7151\n",
      "Test MAE: 0.20844\n",
      "Test MAPE: 2.96866\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# single rnn \n",
    "# from numpy.random import seed\n",
    "# seed(1)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(SimpleRNN(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,kernel_initializer='he_normal',recurrent_initializer='lecun_uniform'))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[3], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('epoch:',i+1)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\treturn model\n",
    "\t\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\t\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\n",
    "\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.4f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.4f' % rmspe_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.4f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\trmspe_test = RMSPE(raw_values[-len(test_scaled):],predictions_test)\n",
    "\tprint('Test RMSPE: %.4f' % rmspe_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel('Time',fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\n",
    "\t \n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back= 4\n",
    "\tneurons=[3]\n",
    "\tn_epoch=100\n",
    "\texperiment(series,look_back,neurons,n_epoch)\n",
    "\n",
    "run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d372fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb7512",
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5633cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rn\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import LSTM\n",
    "#from deap import base, creator, tools, algorithms\n",
    "from keras import regularizers\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "import random\n",
    "\n",
    "\n",
    "def parser(x):\n",
    "\treturn datetime.strptime(x, '%Y%m')\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "\tdiff = list()\n",
    "\tfor i in range(interval, len(dataset)):\n",
    "\t\tvalue = dataset[i] - dataset[i - interval]\n",
    "\t\tdiff.append(value)\n",
    "\treturn Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "\treturn yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "\t# fit scaler\n",
    "\tscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\tscaler = scaler.fit(train)\n",
    "\t# transform train\n",
    "\ttrain = train.reshape(train.shape[0], train.shape[1])\n",
    "\ttrain_scaled = scaler.transform(train)\n",
    "\t# transform test\n",
    "\ttest = test.reshape(test.shape[0], test.shape[1])\n",
    "\ttest_scaled = scaler.transform(test)\n",
    "\treturn scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "\tnew_row = [x for x in X] + [value]\n",
    "\tarray = np.array(new_row)\n",
    "\tarray = array.reshape(1, len(array))\n",
    "\tinverted = scaler.inverse_transform(array)\n",
    "\treturn inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "\tX, y = train[:, 0:-1], train[:, -1]\n",
    "\tX = X.reshape(X.shape[0], X.shape[1],1 )\n",
    "\t\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\tmodel.add(Dropout(0.3))\n",
    "\t#model.add(LSTM(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t#model.add(Dropout(0.3))\n",
    "\t#model.add(LSTM(neurons[2], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t#model.add(Dropout(0.3))\n",
    "\t# model.add(LSTM(neurons[3], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "\t# model.add(Dropout(0.3))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\t\n",
    "\tfor i in range(nb_epoch):\n",
    "\t\tprint('Epoch:',i)\n",
    "\t\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
    "\t\tmodel.reset_states()\n",
    "\t\n",
    "\treturn model\n",
    "\t\n",
    "\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "\tX = X.reshape(1, len(X), 1)\n",
    "\tyhat = model.predict(X, batch_size=batch_size)\n",
    "\treturn yhat[0,0]\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataset = np.insert(dataset,[0]*look_back,0)    \n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back)]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back])\n",
    "\tdataY= np.array(dataY)        \n",
    "\tdataY = np.reshape(dataY,(dataY.shape[0],1))\n",
    "\tdataset = np.concatenate((dataX,dataY),axis=1)  \n",
    "\treturn dataset\n",
    "\n",
    "\n",
    "# compute RMSPE\n",
    "def RMSPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += ((x[i]-y[i])/x[i])**2\n",
    "\tresult /= len(x)\n",
    "\tresult = sqrt(result)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "# compute MAPE\n",
    "def MAPE(x,y):\n",
    "\tresult=0\n",
    "\tfor i in range(len(x)):\n",
    "\t\tresult += abs((x[i]-y[i])/x[i])\n",
    "\tresult /= len(x)\n",
    "\tresult *= 100\n",
    "\treturn result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def experiment(series,look_back,neurons,n_epoch):\n",
    "\n",
    "\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\traw_values = series.values\n",
    "\t# transform data to be stationary\n",
    "\tdiff = difference(raw_values, 1)\n",
    "\t\n",
    "\n",
    "\t# create dataset x,y\n",
    "\tdataset = diff.values\n",
    "\tdataset = create_dataset(dataset,look_back)\n",
    "\n",
    "\n",
    "\t# split into train and test sets\n",
    "\ttrain_size = int(dataset.shape[0] * 0.8)\n",
    "\ttest_size = dataset.shape[0] - train_size\n",
    "\ttrain, test = dataset[0:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "\t# transform the scale of the data\n",
    "\tscaler, train_scaled, test_scaled = scale(train, test)\n",
    "\n",
    "\t\n",
    "\t\n",
    "\t# fit the model\n",
    "\tlstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
    "\t\n",
    "\n",
    "\t# forecast the entire training dataset to build up state for forecasting\n",
    "\tprint('Forecasting Training Data')   \n",
    "\tpredictions_train = list()\n",
    "\tfor i in range(len(train_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_train.append(yhat)\n",
    "\t\texpected = raw_values[ i+1 ] \n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance\n",
    "\trmse_train = sqrt(mean_squared_error(raw_values[1:len(train_scaled)+1], predictions_train))\n",
    "\tprint('Train RMSE: %.5f' % rmse_train)\n",
    "\t#report performance using RMSPE\n",
    "\tRMSPE_train = RMSPE(raw_values[1:len(train_scaled)+1],predictions_train)\n",
    "\tprint('Train RMSPE: %.5f' % RMSPE_train)\n",
    "\tMAE_train = mean_absolute_error(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAE: %.5f' % MAE_train)\n",
    "\tMAPE_train = MAPE(raw_values[1:len(train_scaled)+1], predictions_train)\n",
    "\tprint('Train MAPE: %.5f' % MAPE_train)\n",
    "\n",
    "\t# forecast the test data\n",
    "\tprint('Forecasting Testing Data')\n",
    "\tpredictions_test = list()\n",
    "\tfor i in range(len(test_scaled)):\n",
    "\t\t# make one-step forecast\n",
    "\t\tX, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "\t\tyhat = forecast_lstm(lstm_model, 1, X)\n",
    "\t\t# invert scaling\n",
    "\t\tyhat = invert_scale(scaler, X, yhat)\n",
    "\t\t# invert differencing\n",
    "\t\tyhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "\t\t# store forecast\n",
    "\t\tpredictions_test.append(yhat)\n",
    "\t\texpected = raw_values[len(train) + i + 1]\n",
    "\t\tprint('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n",
    "\n",
    "\t# report performance using RMSE\n",
    "\trmse_test = sqrt(mean_squared_error(raw_values[-len(test_scaled):], predictions_test))\n",
    "\tprint('Test RMSE: %.5f' % rmse_test)\n",
    "\t#report performance using RMSPE\n",
    "\tRMSPE_test = RMSPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test RMSPE: %.5f' % RMSPE_test)\n",
    "\tMAE_test = mean_absolute_error(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAE: %.5f' % MAE_test)\n",
    "\tMAPE_test = MAPE(raw_values[-len(test_scaled):], predictions_test)\n",
    "\tprint('Test MAPE: %.5f' % MAPE_test)\n",
    "\n",
    "\tpredictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
    "\n",
    "\t# line plot of observed vs predicted\n",
    "\tfig, ax = plt.subplots(1)\n",
    "\tax.plot(raw_values, label='original', color='blue')\n",
    "\tax.plot(predictions, label='predictions', color='red')\n",
    "\tax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
    "\tax.legend(loc='upper right')\n",
    "\tax.set_xlabel('Time',fontsize = 16)\n",
    "\tax.set_ylabel('oil production '+ r'$(10^4 m^3)$',fontsize = 16)\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def run():\n",
    "\n",
    "\t#load dataset\n",
    "\tseries = read_csv('oil_production.csv', header=0,parse_dates=[0],index_col=0, squeeze=True,date_parser=parser)\n",
    "\tlook_back=5\n",
    "\tneurons= [ 4 ] \n",
    "\tn_epochs=150\n",
    "\t\n",
    "\t\n",
    "\n",
    "\texperiment(series,look_back,neurons,n_epochs)\n",
    "\n",
    "\n",
    "run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
